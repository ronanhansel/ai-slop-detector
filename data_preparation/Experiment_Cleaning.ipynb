{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6b6d99",
   "metadata": {},
   "source": [
    "# Take all content of post and comment and save to file for further analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa2dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define data directory\n",
    "DATA_DIR = Path(\"h:/Dev/University/DataScience/Project/data_preparation/influencer_data\")\n",
    "OUTPUT_DIR = Path(\"h:/Dev/University/DataScience/Project/data_preparation/outputs\")\n",
    "\n",
    "def get_json_list(dir_path):\n",
    "    \"\"\"Extract content and metadata from a post\"\"\"\n",
    "    \n",
    "    # take list of all json files in the directory dir_path\n",
    "    post_files = [f for f in os.listdir(dir_path) if f.endswith(\".json\")]\n",
    "    print(f\"Found {len(post_files)} post files in {dir_path}\")\n",
    "    print(\"Sample files:\", post_files[:3])\n",
    "    return post_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c8f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_content(json_files):\n",
    "    with open(DATA_DIR / json_files, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract all content for analysis\n",
    "    extracted_content = {\n",
    "        \"posts\": [],\n",
    "        \"comments\": [],\n",
    "        \"statistics\": {\n",
    "            \"total_posts\": len(data),\n",
    "            \"total_comments\": 0,\n",
    "            \"total_nested_comments\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def extract_comment_content(comment, depth=0):\n",
    "        \"\"\"Recursively extract comment content\"\"\"\n",
    "        comment_data = {\n",
    "            \"username\": comment.get(\"username\"),\n",
    "            \"content\": comment.get(\"content\"),\n",
    "            \"likes\": comment.get(\"likes\"),\n",
    "            \"retweet\": comment.get(\"retweet\"),\n",
    "            \"share\": comment.get(\"share\"),\n",
    "            \"views\": comment.get(\"views\"),\n",
    "            \"replies\": comment.get(\"replies\"),\n",
    "            \"link\": comment.get(\"link\"),\n",
    "            \"depth\": depth\n",
    "        }\n",
    "        \n",
    "        extracted_content[\"comments\"].append(comment_data)\n",
    "        \n",
    "        if depth == 0:\n",
    "            extracted_content[\"statistics\"][\"total_comments\"] += 1\n",
    "        else:\n",
    "            extracted_content[\"statistics\"][\"total_nested_comments\"] += 1\n",
    "        \n",
    "        # Process nested replies\n",
    "        if \"replies_content\" in comment and comment[\"replies_content\"]:\n",
    "            for reply in comment[\"replies_content\"]:\n",
    "                extract_comment_content(reply, depth + 1)\n",
    "\n",
    "    # Process each post\n",
    "    for post in data:\n",
    "        post_data = {\n",
    "            \"username\": post.get(\"username\"),\n",
    "            \"content\": post.get(\"content\"),\n",
    "            \"likes\": post.get(\"likes\"),\n",
    "            \"retweet\": post.get(\"retweet\"),\n",
    "            \"share\": post.get(\"share\"),\n",
    "            \"views\": post.get(\"views\"),\n",
    "            \"replies\": post.get(\"replies\"),\n",
    "            \"link\": post.get(\"link\"),\n",
    "            \"num_direct_comments\": len(post.get(\"replies_content\", []))\n",
    "        }\n",
    "        \n",
    "        extracted_content[\"posts\"].append(post_data)\n",
    "        \n",
    "        # Process all comments for this post\n",
    "        if \"replies_content\" in post and post[\"replies_content\"]:\n",
    "            for comment in post[\"replies_content\"]:\n",
    "                extract_comment_content(comment, depth=0)\n",
    "\n",
    "    # Save extracted content\n",
    "    output_file = OUTPUT_DIR / json_files.replace(\".json\", \"_extracted_content.json\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(extracted_content, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Content extracted and saved to: {output_file}\")\n",
    "    print(f\"\\nðŸ“Š Statistics:\")\n",
    "    print(f\"   - Total posts: {extracted_content['statistics']['total_posts']}\")\n",
    "    print(f\"   - Total direct comments: {extracted_content['statistics']['total_comments']}\")\n",
    "    print(f\"   - Total nested comments: {extracted_content['statistics']['total_nested_comments']}\")\n",
    "    print(f\"   - Total all comments: {extracted_content['statistics']['total_comments'] + extracted_content['statistics']['total_nested_comments']}\")\n",
    "\n",
    "    # Also create a flat text file with all content for easy reading\n",
    "    posts_file = OUTPUT_DIR / \"post\" / json_files.replace(\".json\", \"_all_content.txt\")\n",
    "    comments_file = OUTPUT_DIR / \"comments\" / json_files.replace(\".json\", \"_all_content.txt\")\n",
    "    # posts_file = OUTPUT_DIR / \"all_posts_content.txt\"\n",
    "    # comments_file = OUTPUT_DIR / \"all_comments_content.txt\"\n",
    "\n",
    "    # lists to accumulate flat lines for this file\n",
    "    post_lines = []\n",
    "    comment_lines = []\n",
    "\n",
    "    # Extract all content for analysis (kept for JSON/summary outputs)\n",
    "    extracted_content = {\n",
    "        \"posts\": [],\n",
    "        \"comments\": [],\n",
    "        \"statistics\": {\n",
    "            \"total_posts\": len(data),\n",
    "            \"total_comments\": 0,\n",
    "            \"total_nested_comments\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def extract_comment_content(comment, depth=0):\n",
    "        \"\"\"Recursively extract comment content\"\"\"\n",
    "        comment_data = {\n",
    "            \"username\": comment.get(\"username\"),\n",
    "            \"content\": comment.get(\"content\"),\n",
    "            \"likes\": comment.get(\"likes\"),\n",
    "            \"retweet\": comment.get(\"retweet\"),\n",
    "            \"share\": comment.get(\"share\"),\n",
    "            \"views\": comment.get(\"views\"),\n",
    "            \"replies\": comment.get(\"replies\"),\n",
    "            \"link\": comment.get(\"link\"),\n",
    "            \"depth\": depth\n",
    "        }\n",
    "\n",
    "        # add flat comment content (single line)\n",
    "        c = (comment.get(\"content\") or \"\").strip()\n",
    "        if c:\n",
    "            comment_lines.append(\" \".join(c.splitlines()))\n",
    "\n",
    "        extracted_content[\"comments\"].append(comment_data)\n",
    "\n",
    "        if depth == 0:\n",
    "            extracted_content[\"statistics\"][\"total_comments\"] += 1\n",
    "        else:\n",
    "            extracted_content[\"statistics\"][\"total_nested_comments\"] += 1\n",
    "\n",
    "        # Process nested replies\n",
    "        if \"replies_content\" in comment and comment[\"replies_content\"]:\n",
    "            for reply in comment[\"replies_content\"]:\n",
    "                extract_comment_content(reply, depth + 1)\n",
    "\n",
    "    # Process each post\n",
    "    for post in data:\n",
    "        # collect flat post content (single line)\n",
    "        p = (post.get(\"content\") or \"\").strip()\n",
    "        if p:\n",
    "            post_lines.append(\" \".join(p.splitlines()))\n",
    "\n",
    "        post_data = {\n",
    "            \"username\": post.get(\"username\"),\n",
    "            \"content\": post.get(\"content\"),\n",
    "            \"likes\": post.get(\"likes\"),\n",
    "            \"retweet\": post.get(\"retweet\"),\n",
    "            \"share\": post.get(\"share\"),\n",
    "            \"views\": post.get(\"views\"),\n",
    "            \"replies\": post.get(\"replies\"),\n",
    "            \"link\": post.get(\"link\"),\n",
    "            \"num_direct_comments\": len(post.get(\"replies_content\", []))\n",
    "        }\n",
    "\n",
    "        extracted_content[\"posts\"].append(post_data)\n",
    "\n",
    "        # Process all comments for this post\n",
    "        if \"replies_content\" in post and post[\"replies_content\"]:\n",
    "            for comment in post[\"replies_content\"]:\n",
    "                extract_comment_content(comment, depth=0)\n",
    "\n",
    "    # Save extracted JSON as before\n",
    "    output_file = OUTPUT_DIR / json_files.replace(\".json\", \"_extracted_content.json\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(extracted_content, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Append flat lines to global text files (one content per line)\n",
    "    with open(posts_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        for line in post_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    with open(comments_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        for line in comment_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "    # Create summary statistics file\n",
    "    summary_file = OUTPUT_DIR / json_files.replace(\".json\", \"_summary.json\")\n",
    "    summary = {\n",
    "        \"total_posts\": len(data),\n",
    "        \"posts_with_comments\": sum(1 for post in data if post.get(\"replies_content\")),\n",
    "        \"total_direct_comments\": extracted_content['statistics']['total_comments'],\n",
    "        \"total_nested_comments\": extracted_content['statistics']['total_nested_comments'],\n",
    "        \"engagement_stats\": {\n",
    "            \"total_likes\": sum(post.get(\"likes\", 0) for post in data),\n",
    "            \"total_retweets\": sum(post.get(\"retweet\", 0) for post in data),\n",
    "            \"total_views\": sum(post.get(\"views\", 0) for post in data if post.get(\"views\")),\n",
    "            \"avg_likes_per_post\": sum(post.get(\"likes\", 0) for post in data) / len(data),\n",
    "            \"avg_views_per_post\": sum(post.get(\"views\", 0) for post in data if post.get(\"views\")) / len([p for p in data if p.get(\"views\")])\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"âœ… Summary statistics saved to: {summary_file}\")\n",
    "    print(f\"\\nðŸ“ˆ Engagement Overview:\")\n",
    "    print(f\"   - Total likes across all posts: {summary['engagement_stats']['total_likes']:,}\")\n",
    "    print(f\"   - Total retweets: {summary['engagement_stats']['total_retweets']:,}\")\n",
    "    print(f\"   - Total views: {summary['engagement_stats']['total_views']:,}\")\n",
    "    print(f\"   - Average likes per post: {summary['engagement_stats']['avg_likes_per_post']:.0f}\")\n",
    "    print(f\"   - Average views per post: {summary['engagement_stats']['avg_views_per_post']:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be88c060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 post files in h:\\Dev\\University\\DataScience\\Project\\data_preparation\\influencer_data\n",
      "Sample files: ['AdamParkhomenko.json', 'ArielleScarcell.json', 'AstroTerry.json']\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\AdamParkhomenko_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 40\n",
      "   - Total direct comments: 647\n",
      "   - Total nested comments: 992\n",
      "   - Total all comments: 1639\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\AdamParkhomenko_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 216,102\n",
      "   - Total retweets: 44,952\n",
      "   - Total views: 6,251,398\n",
      "   - Average likes per post: 5403\n",
      "   - Average views per post: 156,285\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ArielleScarcell_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 33\n",
      "   - Total direct comments: 359\n",
      "   - Total nested comments: 2027\n",
      "   - Total all comments: 2386\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ArielleScarcell_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 454,504\n",
      "   - Total retweets: 111,958\n",
      "   - Total views: 42,272,488\n",
      "   - Average likes per post: 13773\n",
      "   - Average views per post: 1,280,984\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\AstroTerry_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 32\n",
      "   - Total direct comments: 511\n",
      "   - Total nested comments: 1130\n",
      "   - Total all comments: 1641\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\AstroTerry_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 316,086\n",
      "   - Total retweets: 50,909\n",
      "   - Total views: 28,247,086\n",
      "   - Average likes per post: 9878\n",
      "   - Average views per post: 882,721\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\bariweiss_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 39\n",
      "   - Total direct comments: 597\n",
      "   - Total nested comments: 777\n",
      "   - Total all comments: 1374\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\bariweiss_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 81,104\n",
      "   - Total retweets: 24,704\n",
      "   - Total views: 25,998,144\n",
      "   - Average likes per post: 2080\n",
      "   - Average views per post: 666,619\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\blackintheempir_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 27\n",
      "   - Total direct comments: 292\n",
      "   - Total nested comments: 164\n",
      "   - Total all comments: 456\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\blackintheempir_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 12,038\n",
      "   - Total retweets: 5,378\n",
      "   - Total views: 1,913,349\n",
      "   - Average likes per post: 446\n",
      "   - Average views per post: 70,865\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\bresreports_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 36\n",
      "   - Total direct comments: 315\n",
      "   - Total nested comments: 990\n",
      "   - Total all comments: 1305\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\bresreports_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 34,725\n",
      "   - Total retweets: 18,266\n",
      "   - Total views: 18,486,154\n",
      "   - Average likes per post: 965\n",
      "   - Average views per post: 513,504\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\brithume_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 46\n",
      "   - Total direct comments: 861\n",
      "   - Total nested comments: 2611\n",
      "   - Total all comments: 3472\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\brithume_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 345,756\n",
      "   - Total retweets: 64,616\n",
      "   - Total views: 57,528,107\n",
      "   - Average likes per post: 7516\n",
      "   - Average views per post: 1,250,611\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\burgessev_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 26\n",
      "   - Total direct comments: 195\n",
      "   - Total nested comments: 238\n",
      "   - Total all comments: 433\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\burgessev_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 21,576\n",
      "   - Total retweets: 5,402\n",
      "   - Total views: 2,292,949\n",
      "   - Average likes per post: 830\n",
      "   - Average views per post: 88,190\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ByYourLogic_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 42\n",
      "   - Total direct comments: 611\n",
      "   - Total nested comments: 673\n",
      "   - Total all comments: 1284\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ByYourLogic_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 223,960\n",
      "   - Total retweets: 18,236\n",
      "   - Total views: 21,574,180\n",
      "   - Average likes per post: 5332\n",
      "   - Average views per post: 513,671\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\CarlHigbie_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 37\n",
      "   - Total direct comments: 928\n",
      "   - Total nested comments: 2835\n",
      "   - Total all comments: 3763\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\CarlHigbie_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 321,575\n",
      "   - Total retweets: 62,458\n",
      "   - Total views: 17,692,542\n",
      "   - Average likes per post: 8691\n",
      "   - Average views per post: 478,177\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\CensoredMen_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 26\n",
      "   - Total direct comments: 752\n",
      "   - Total nested comments: 1400\n",
      "   - Total all comments: 2152\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\CensoredMen_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 397,540\n",
      "   - Total retweets: 162,051\n",
      "   - Total views: 27,778,306\n",
      "   - Average likes per post: 15290\n",
      "   - Average views per post: 1,068,396\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ChrisLoesch_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 42\n",
      "   - Total direct comments: 347\n",
      "   - Total nested comments: 465\n",
      "   - Total all comments: 812\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ChrisLoesch_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 68,025\n",
      "   - Total retweets: 18,949\n",
      "   - Total views: 4,604,091\n",
      "   - Average likes per post: 1620\n",
      "   - Average views per post: 109,621\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\CollinRugg_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 21\n",
      "   - Total direct comments: 696\n",
      "   - Total nested comments: 1614\n",
      "   - Total all comments: 2310\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\CollinRugg_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 502,104\n",
      "   - Total retweets: 72,397\n",
      "   - Total views: 33,142,946\n",
      "   - Average likes per post: 23910\n",
      "   - Average views per post: 1,578,236\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\danpfeiffer_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 47\n",
      "   - Total direct comments: 959\n",
      "   - Total nested comments: 1713\n",
      "   - Total all comments: 2672\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\danpfeiffer_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 290,056\n",
      "   - Total retweets: 48,006\n",
      "   - Total views: 39,154,059\n",
      "   - Average likes per post: 6171\n",
      "   - Average views per post: 833,065\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\danprimack_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 27\n",
      "   - Total direct comments: 94\n",
      "   - Total nested comments: 32\n",
      "   - Total all comments: 126\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\danprimack_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 1,130\n",
      "   - Total retweets: 394\n",
      "   - Total views: 2,617,385\n",
      "   - Average likes per post: 42\n",
      "   - Average views per post: 96,940\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\dlacalle_IA_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 27\n",
      "   - Total direct comments: 180\n",
      "   - Total nested comments: 279\n",
      "   - Total all comments: 459\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\dlacalle_IA_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 19,387\n",
      "   - Total retweets: 4,097\n",
      "   - Total views: 3,437,964\n",
      "   - Average likes per post: 718\n",
      "   - Average views per post: 132,229\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\DrLoupis_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 43\n",
      "   - Total direct comments: 668\n",
      "   - Total nested comments: 1114\n",
      "   - Total all comments: 1782\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\DrLoupis_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 88,020\n",
      "   - Total retweets: 21,085\n",
      "   - Total views: 12,200,108\n",
      "   - Average likes per post: 2047\n",
      "   - Average views per post: 283,723\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\elonmusk_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 44\n",
      "   - Total direct comments: 1035\n",
      "   - Total nested comments: 4226\n",
      "   - Total all comments: 5261\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\elonmusk_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 1,044,304\n",
      "   - Total retweets: 203,791\n",
      "   - Total views: 283,452,299\n",
      "   - Average likes per post: 23734\n",
      "   - Average views per post: 6,442,098\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\eveforamerica_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 31\n",
      "   - Total direct comments: 450\n",
      "   - Total nested comments: 686\n",
      "   - Total all comments: 1136\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\eveforamerica_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 229,681\n",
      "   - Total retweets: 39,604\n",
      "   - Total views: 16,492,057\n",
      "   - Average likes per post: 7409\n",
      "   - Average views per post: 532,002\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\FiorellaIsabelM_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 35\n",
      "   - Total direct comments: 382\n",
      "   - Total nested comments: 798\n",
      "   - Total all comments: 1180\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\FiorellaIsabelM_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 77,585\n",
      "   - Total retweets: 32,389\n",
      "   - Total views: 9,342,977\n",
      "   - Average likes per post: 2217\n",
      "   - Average views per post: 266,942\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\FrankDangelo23_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 45\n",
      "   - Total direct comments: 571\n",
      "   - Total nested comments: 2054\n",
      "   - Total all comments: 2625\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\FrankDangelo23_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 272,020\n",
      "   - Total retweets: 132,851\n",
      "   - Total views: 40,770,196\n",
      "   - Average likes per post: 6045\n",
      "   - Average views per post: 906,004\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\garethicke_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 36\n",
      "   - Total direct comments: 691\n",
      "   - Total nested comments: 1586\n",
      "   - Total all comments: 2277\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\garethicke_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 216,110\n",
      "   - Total retweets: 113,555\n",
      "   - Total views: 10,879,985\n",
      "   - Average likes per post: 6003\n",
      "   - Average views per post: 302,222\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\GregRubini_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 40\n",
      "   - Total direct comments: 606\n",
      "   - Total nested comments: 4790\n",
      "   - Total all comments: 5396\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\GregRubini_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 1,679,908\n",
      "   - Total retweets: 697,085\n",
      "   - Total views: 486,575,043\n",
      "   - Average likes per post: 41998\n",
      "   - Average views per post: 12,804,606\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\HilzFuld_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 26\n",
      "   - Total direct comments: 475\n",
      "   - Total nested comments: 533\n",
      "   - Total all comments: 1008\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\HilzFuld_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 43,597\n",
      "   - Total retweets: 7,317\n",
      "   - Total views: 29,509,155\n",
      "   - Average likes per post: 1677\n",
      "   - Average views per post: 1,134,968\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\IamBrookJackson_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 38\n",
      "   - Total direct comments: 633\n",
      "   - Total nested comments: 257\n",
      "   - Total all comments: 890\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\IamBrookJackson_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 38,777\n",
      "   - Total retweets: 24,349\n",
      "   - Total views: 3,035,509\n",
      "   - Average likes per post: 1020\n",
      "   - Average views per post: 79,882\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\jayrosen_nyu_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 26\n",
      "   - Total direct comments: 208\n",
      "   - Total nested comments: 238\n",
      "   - Total all comments: 446\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\jayrosen_nyu_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 8,728\n",
      "   - Total retweets: 1,732\n",
      "   - Total views: 1,397,963\n",
      "   - Average likes per post: 336\n",
      "   - Average views per post: 53,768\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\jimsciutto_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 32\n",
      "   - Total direct comments: 652\n",
      "   - Total nested comments: 1014\n",
      "   - Total all comments: 1666\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\jimsciutto_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 90,143\n",
      "   - Total retweets: 29,893\n",
      "   - Total views: 13,207,426\n",
      "   - Average likes per post: 2817\n",
      "   - Average views per post: 412,732\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JoeConchaTV_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 37\n",
      "   - Total direct comments: 958\n",
      "   - Total nested comments: 2155\n",
      "   - Total all comments: 3113\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JoeConchaTV_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 370,362\n",
      "   - Total retweets: 132,671\n",
      "   - Total views: 22,496,148\n",
      "   - Average likes per post: 10010\n",
      "   - Average views per post: 608,004\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JonahDispatch_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 42\n",
      "   - Total direct comments: 536\n",
      "   - Total nested comments: 438\n",
      "   - Total all comments: 974\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JonahDispatch_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 119,660\n",
      "   - Total retweets: 27,170\n",
      "   - Total views: 11,327,491\n",
      "   - Average likes per post: 2849\n",
      "   - Average views per post: 269,702\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JonathanTurley_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 21\n",
      "   - Total direct comments: 605\n",
      "   - Total nested comments: 314\n",
      "   - Total all comments: 919\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JonathanTurley_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 35,014\n",
      "   - Total retweets: 7,059\n",
      "   - Total views: 3,382,242\n",
      "   - Average likes per post: 1667\n",
      "   - Average views per post: 161,059\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JoshDenny_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 23\n",
      "   - Total direct comments: 178\n",
      "   - Total nested comments: 417\n",
      "   - Total all comments: 595\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\JoshDenny_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 57,208\n",
      "   - Total retweets: 7,716\n",
      "   - Total views: 2,585,834\n",
      "   - Average likes per post: 2487\n",
      "   - Average views per post: 117,538\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\kacdnp91_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 30\n",
      "   - Total direct comments: 292\n",
      "   - Total nested comments: 562\n",
      "   - Total all comments: 854\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\kacdnp91_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 16,248\n",
      "   - Total retweets: 4,759\n",
      "   - Total views: 1,381,840\n",
      "   - Average likes per post: 542\n",
      "   - Average views per post: 46,061\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\KatiePhang_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 43\n",
      "   - Total direct comments: 729\n",
      "   - Total nested comments: 1349\n",
      "   - Total all comments: 2078\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\KatiePhang_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 211,587\n",
      "   - Total retweets: 55,133\n",
      "   - Total views: 22,350,011\n",
      "   - Average likes per post: 4921\n",
      "   - Average views per post: 519,768\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\kristina_wong_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 41\n",
      "   - Total direct comments: 738\n",
      "   - Total nested comments: 1990\n",
      "   - Total all comments: 2728\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\kristina_wong_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 387,176\n",
      "   - Total retweets: 114,542\n",
      "   - Total views: 33,777,755\n",
      "   - Average likes per post: 9443\n",
      "   - Average views per post: 823,848\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\kyledcheney_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 26\n",
      "   - Total direct comments: 471\n",
      "   - Total nested comments: 822\n",
      "   - Total all comments: 1293\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\kyledcheney_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 51,583\n",
      "   - Total retweets: 15,603\n",
      "   - Total views: 4,422,230\n",
      "   - Average likes per post: 1984\n",
      "   - Average views per post: 170,086\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\laurashin_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 35\n",
      "   - Total direct comments: 405\n",
      "   - Total nested comments: 274\n",
      "   - Total all comments: 679\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\laurashin_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 28,111\n",
      "   - Total retweets: 5,377\n",
      "   - Total views: 6,793,095\n",
      "   - Average likes per post: 803\n",
      "   - Average views per post: 199,797\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\LEBassett_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 41\n",
      "   - Total direct comments: 703\n",
      "   - Total nested comments: 4097\n",
      "   - Total all comments: 4800\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\LEBassett_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 1,847,210\n",
      "   - Total retweets: 330,362\n",
      "   - Total views: 167,178,489\n",
      "   - Average likes per post: 45054\n",
      "   - Average views per post: 4,077,524\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\LeftAtLondon_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 2\n",
      "   - Total direct comments: 1\n",
      "   - Total nested comments: 0\n",
      "   - Total all comments: 1\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\LeftAtLondon_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 198\n",
      "   - Total retweets: 3\n",
      "   - Total views: 10,347\n",
      "   - Average likes per post: 99\n",
      "   - Average views per post: 5,174\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Leslieoo7_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 36\n",
      "   - Total direct comments: 617\n",
      "   - Total nested comments: 1562\n",
      "   - Total all comments: 2179\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Leslieoo7_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 495,024\n",
      "   - Total retweets: 113,079\n",
      "   - Total views: 30,834,663\n",
      "   - Average likes per post: 13751\n",
      "   - Average views per post: 856,518\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\LibertyCappy_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 30\n",
      "   - Total direct comments: 409\n",
      "   - Total nested comments: 175\n",
      "   - Total all comments: 584\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\LibertyCappy_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 7,851\n",
      "   - Total retweets: 1,222\n",
      "   - Total views: 2,255,455\n",
      "   - Average likes per post: 262\n",
      "   - Average views per post: 75,182\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MarchandSurgery_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 33\n",
      "   - Total direct comments: 614\n",
      "   - Total nested comments: 2629\n",
      "   - Total all comments: 3243\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MarchandSurgery_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 247,063\n",
      "   - Total retweets: 44,034\n",
      "   - Total views: 11,726,758\n",
      "   - Average likes per post: 7487\n",
      "   - Average views per post: 355,356\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MarkHertling_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 32\n",
      "   - Total direct comments: 563\n",
      "   - Total nested comments: 1596\n",
      "   - Total all comments: 2159\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MarkHertling_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 181,946\n",
      "   - Total retweets: 32,746\n",
      "   - Total views: 16,258,858\n",
      "   - Average likes per post: 5686\n",
      "   - Average views per post: 508,089\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MaryLTrump_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 20\n",
      "   - Total direct comments: 393\n",
      "   - Total nested comments: 627\n",
      "   - Total all comments: 1020\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MaryLTrump_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 45,679\n",
      "   - Total retweets: 8,982\n",
      "   - Total views: 1,413,733\n",
      "   - Average likes per post: 2284\n",
      "   - Average views per post: 70,687\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MattBruenig_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 38\n",
      "   - Total direct comments: 225\n",
      "   - Total nested comments: 562\n",
      "   - Total all comments: 787\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MattBruenig_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 59,617\n",
      "   - Total retweets: 8,538\n",
      "   - Total views: 10,724,965\n",
      "   - Average likes per post: 1569\n",
      "   - Average views per post: 282,236\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MikeASperrazza_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 27\n",
      "   - Total direct comments: 306\n",
      "   - Total nested comments: 1687\n",
      "   - Total all comments: 1993\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MikeASperrazza_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 284,485\n",
      "   - Total retweets: 82,861\n",
      "   - Total views: 11,367,033\n",
      "   - Average likes per post: 10536\n",
      "   - Average views per post: 437,194\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MikeSington_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 21\n",
      "   - Total direct comments: 545\n",
      "   - Total nested comments: 836\n",
      "   - Total all comments: 1381\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MikeSington_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 552,689\n",
      "   - Total retweets: 109,624\n",
      "   - Total views: 5,036,276\n",
      "   - Average likes per post: 26319\n",
      "   - Average views per post: 251,814\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\molly0xFFF_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 23\n",
      "   - Total direct comments: 198\n",
      "   - Total nested comments: 344\n",
      "   - Total all comments: 542\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\molly0xFFF_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 82,334\n",
      "   - Total retweets: 12,242\n",
      "   - Total views: 11,590,974\n",
      "   - Average likes per post: 3580\n",
      "   - Average views per post: 503,955\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MsAvaArmstrong_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 40\n",
      "   - Total direct comments: 663\n",
      "   - Total nested comments: 565\n",
      "   - Total all comments: 1228\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\MsAvaArmstrong_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 268,273\n",
      "   - Total retweets: 83,172\n",
      "   - Total views: 14,374,949\n",
      "   - Average likes per post: 6707\n",
      "   - Average views per post: 359,374\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\NAChristakis_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 34\n",
      "   - Total direct comments: 318\n",
      "   - Total nested comments: 801\n",
      "   - Total all comments: 1119\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\NAChristakis_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 185,158\n",
      "   - Total retweets: 26,689\n",
      "   - Total views: 42,000,807\n",
      "   - Average likes per post: 5446\n",
      "   - Average views per post: 1,235,318\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\omgno2trump_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 31\n",
      "   - Total direct comments: 376\n",
      "   - Total nested comments: 1338\n",
      "   - Total all comments: 1714\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\omgno2trump_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 795,448\n",
      "   - Total retweets: 190,574\n",
      "   - Total views: 46,351,959\n",
      "   - Average likes per post: 25660\n",
      "   - Average views per post: 1,495,224\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Prolotario1_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 33\n",
      "   - Total direct comments: 997\n",
      "   - Total nested comments: 870\n",
      "   - Total all comments: 1867\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Prolotario1_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 88,374\n",
      "   - Total retweets: 19,808\n",
      "   - Total views: 7,633,629\n",
      "   - Average likes per post: 2678\n",
      "   - Average views per post: 231,322\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Rach_IC_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 39\n",
      "   - Total direct comments: 200\n",
      "   - Total nested comments: 526\n",
      "   - Total all comments: 726\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Rach_IC_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 103,313\n",
      "   - Total retweets: 42,082\n",
      "   - Total views: 8,094,853\n",
      "   - Average likes per post: 2649\n",
      "   - Average views per post: 207,560\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\realDonaldTrump_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 22\n",
      "   - Total direct comments: 756\n",
      "   - Total nested comments: 13749\n",
      "   - Total all comments: 14505\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\realDonaldTrump_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 8,062,340\n",
      "   - Total retweets: 1,024,131\n",
      "   - Total views: 930,477,000\n",
      "   - Average likes per post: 366470\n",
      "   - Average views per post: 42,294,409\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\RightWingCope_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 29\n",
      "   - Total direct comments: 887\n",
      "   - Total nested comments: 2292\n",
      "   - Total all comments: 3179\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\RightWingCope_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 504,032\n",
      "   - Total retweets: 43,754\n",
      "   - Total views: 58,310,652\n",
      "   - Average likes per post: 17380\n",
      "   - Average views per post: 2,010,712\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\RogerJStoneJr_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 29\n",
      "   - Total direct comments: 436\n",
      "   - Total nested comments: 167\n",
      "   - Total all comments: 603\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\RogerJStoneJr_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 138,742\n",
      "   - Total retweets: 40,036\n",
      "   - Total views: 10,315,878\n",
      "   - Average likes per post: 4784\n",
      "   - Average views per post: 355,720\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\saifedean_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 42\n",
      "   - Total direct comments: 535\n",
      "   - Total nested comments: 607\n",
      "   - Total all comments: 1142\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\saifedean_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 70,341\n",
      "   - Total retweets: 55,740\n",
      "   - Total views: 6,376,271\n",
      "   - Average likes per post: 1675\n",
      "   - Average views per post: 151,816\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\SamParkerSenate_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 38\n",
      "   - Total direct comments: 567\n",
      "   - Total nested comments: 1766\n",
      "   - Total all comments: 2333\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\SamParkerSenate_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 151,497\n",
      "   - Total retweets: 57,197\n",
      "   - Total views: 39,837,376\n",
      "   - Average likes per post: 3987\n",
      "   - Average views per post: 1,048,352\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\SarahTheHaider_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 39\n",
      "   - Total direct comments: 613\n",
      "   - Total nested comments: 1342\n",
      "   - Total all comments: 1955\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\SarahTheHaider_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 306,334\n",
      "   - Total retweets: 44,806\n",
      "   - Total views: 40,722,323\n",
      "   - Average likes per post: 7855\n",
      "   - Average views per post: 1,044,162\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\secupp_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 40\n",
      "   - Total direct comments: 579\n",
      "   - Total nested comments: 2317\n",
      "   - Total all comments: 2896\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\secupp_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 352,077\n",
      "   - Total retweets: 84,025\n",
      "   - Total views: 89,104,370\n",
      "   - Average likes per post: 8802\n",
      "   - Average views per post: 2,284,727\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\simon_schama_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 43\n",
      "   - Total direct comments: 748\n",
      "   - Total nested comments: 1635\n",
      "   - Total all comments: 2383\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\simon_schama_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 352,701\n",
      "   - Total retweets: 147,069\n",
      "   - Total views: 20,070,575\n",
      "   - Average likes per post: 8202\n",
      "   - Average views per post: 466,758\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\StellaParton_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 17\n",
      "   - Total direct comments: 451\n",
      "   - Total nested comments: 361\n",
      "   - Total all comments: 812\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\StellaParton_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 10,700\n",
      "   - Total retweets: 1,957\n",
      "   - Total views: 410,165\n",
      "   - Average likes per post: 629\n",
      "   - Average views per post: 24,127\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Tatarigami_UA_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 26\n",
      "   - Total direct comments: 399\n",
      "   - Total nested comments: 180\n",
      "   - Total all comments: 579\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\Tatarigami_UA_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 38,059\n",
      "   - Total retweets: 7,979\n",
      "   - Total views: 6,204,575\n",
      "   - Average likes per post: 1464\n",
      "   - Average views per post: 238,638\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\thatdayin1992_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 29\n",
      "   - Total direct comments: 392\n",
      "   - Total nested comments: 482\n",
      "   - Total all comments: 874\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\thatdayin1992_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 380,828\n",
      "   - Total retweets: 57,903\n",
      "   - Total views: 49,155,038\n",
      "   - Average likes per post: 13132\n",
      "   - Average views per post: 1,695,001\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\TheBigMigShow_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 28\n",
      "   - Total direct comments: 133\n",
      "   - Total nested comments: 173\n",
      "   - Total all comments: 306\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\TheBigMigShow_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 15,160\n",
      "   - Total retweets: 6,032\n",
      "   - Total views: 5,143,253\n",
      "   - Average likes per post: 541\n",
      "   - Average views per post: 183,688\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\thecoastguy_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 38\n",
      "   - Total direct comments: 591\n",
      "   - Total nested comments: 1406\n",
      "   - Total all comments: 1997\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\thecoastguy_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 167,022\n",
      "   - Total retweets: 79,068\n",
      "   - Total views: 16,839,540\n",
      "   - Average likes per post: 4395\n",
      "   - Average views per post: 443,146\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\thejackhopkins_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 36\n",
      "   - Total direct comments: 103\n",
      "   - Total nested comments: 654\n",
      "   - Total all comments: 757\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\thejackhopkins_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 49,893\n",
      "   - Total retweets: 7,699\n",
      "   - Total views: 1,748,658\n",
      "   - Average likes per post: 1386\n",
      "   - Average views per post: 48,574\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\timinhonolulu_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 41\n",
      "   - Total direct comments: 363\n",
      "   - Total nested comments: 266\n",
      "   - Total all comments: 629\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\timinhonolulu_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 36,972\n",
      "   - Total retweets: 16,462\n",
      "   - Total views: 3,714,617\n",
      "   - Average likes per post: 902\n",
      "   - Average views per post: 92,865\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\WBrettWilson_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 37\n",
      "   - Total direct comments: 570\n",
      "   - Total nested comments: 720\n",
      "   - Total all comments: 1290\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\WBrettWilson_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 131,512\n",
      "   - Total retweets: 43,320\n",
      "   - Total views: 6,405,927\n",
      "   - Average likes per post: 3554\n",
      "   - Average views per post: 173,133\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\WhiteHouse_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 184\n",
      "   - Total direct comments: 4310\n",
      "   - Total nested comments: 7618\n",
      "   - Total all comments: 11928\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\WhiteHouse_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 2,988,603\n",
      "   - Total retweets: 728,725\n",
      "   - Total views: 214,180,384\n",
      "   - Average likes per post: 16242\n",
      "   - Average views per post: 1,164,024\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\yarahawari_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 38\n",
      "   - Total direct comments: 529\n",
      "   - Total nested comments: 520\n",
      "   - Total all comments: 1049\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\yarahawari_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 64,197\n",
      "   - Total retweets: 59,708\n",
      "   - Total views: 12,309,958\n",
      "   - Average likes per post: 1689\n",
      "   - Average views per post: 323,946\n",
      "âœ… Content extracted and saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ylecun_extracted_content.json\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "   - Total posts: 44\n",
      "   - Total direct comments: 620\n",
      "   - Total nested comments: 1043\n",
      "   - Total all comments: 1663\n",
      "âœ… Summary statistics saved to: h:\\Dev\\University\\DataScience\\Project\\data_preparation\\outputs\\ylecun_summary.json\n",
      "\n",
      "ðŸ“ˆ Engagement Overview:\n",
      "   - Total likes across all posts: 85,502\n",
      "   - Total retweets: 50,351\n",
      "   - Total views: 10,271,239\n",
      "   - Average likes per post: 1943\n",
      "   - Average views per post: 233,437\n"
     ]
    }
   ],
   "source": [
    "all_json_files = get_json_list(DATA_DIR)\n",
    "for json_file in all_json_files:\n",
    "    try:\n",
    "        collect_all_content(json_file)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {json_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef3b6b",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Link in a tweet is hided behind a structure https://t.co/xxxxx. We need to extract the full link to analyze what it links to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1fc093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def expand_twitter_link(url):\n",
    "    try:\n",
    "        r = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        return r.url\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011833e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/mrlourage/status/1975311155112607859/photo/1\n"
     ]
    }
   ],
   "source": [
    "link = \" https://t.co/YwR0scMf5F\"\n",
    "print(expand_twitter_link(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf703be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "def extract_and_resolve_links(comments_path, posts_path, output_links):\n",
    "    \"\"\"\n",
    "    Extract all links from text files in comments_path and posts_path directories,\n",
    "    resolve shortened URLs, and save to CSV file incrementally.\n",
    "    \n",
    "    Args:\n",
    "        comments_path: Path to directory containing comment text files\n",
    "        posts_path: Path to directory containing post text files\n",
    "        output_links: Path to output directory for links CSV\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    output_dir = Path(output_links)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Output CSV file and progress tracker\n",
    "    csv_file = output_dir / \"links.csv\"\n",
    "    progress_file = output_dir / \"progress.json\"\n",
    "    \n",
    "    # Load completed files from previous run\n",
    "    completed_files = set()\n",
    "    if progress_file.exists():\n",
    "        with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            completed_files = set(json.load(f).get(\"completed_files\", []))\n",
    "        print(f\"ðŸ“‹ Resuming from previous run. {len(completed_files)} files already processed.\")\n",
    "    \n",
    "    # Check if CSV exists, if not create with header\n",
    "    csv_exists = csv_file.exists()\n",
    "    \n",
    "    # Regular expression to find URLs\n",
    "    url_pattern = re.compile(r'https?://[^\\s]+')\n",
    "    \n",
    "    def process_directory(dir_path, category, csv_exists=csv_exists):\n",
    "        dir_path = Path(dir_path)\n",
    "        if not dir_path.exists():\n",
    "            print(f\"âš ï¸  Directory not found: {dir_path}\")\n",
    "            return\n",
    "\n",
    "        txt_files = list(dir_path.glob(\"*.txt\"))\n",
    "        # Filter out already completed files\n",
    "        txt_files = [f for f in txt_files if f.name not in completed_files]\n",
    "        \n",
    "        if not txt_files:\n",
    "            print(f\"âœ… All files in {dir_path} ({category}) already processed!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nProcessing {len(txt_files)} files in {dir_path} ({category})\")\n",
    "\n",
    "        for txt_file in tqdm(txt_files, desc=f\"Processing {category}s\", unit=\"file\", position=0):\n",
    "            try:\n",
    "                with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                urls = url_pattern.findall(content)\n",
    "                \n",
    "                # Collect links for this file\n",
    "                file_links = []\n",
    "                for url in tqdm(urls, desc=f\"  Resolving links in {txt_file.name}\", leave=False, unit=\"link\", position=1):\n",
    "                    url = url.rstrip('.,;:!?)')\n",
    "                    resolved_url = expand_twitter_link(url)\n",
    "                    file_links.append({\n",
    "                        \"link\": url,\n",
    "                        \"resolved_link\": resolved_url if resolved_url else url,\n",
    "                        \"category\": category,\n",
    "                        \"source_file\": txt_file.name\n",
    "                    })\n",
    "                \n",
    "                # Append to CSV immediately after processing each file\n",
    "                with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    fieldnames = [\"link\", \"resolved_link\", \"category\", \"source_file\"]\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    \n",
    "                    # Write header only if file is new\n",
    "                    if not csv_exists:\n",
    "                        writer.writeheader()\n",
    "                        csv_exists = True\n",
    "                    \n",
    "                    writer.writerows(file_links)\n",
    "                \n",
    "                # Mark file as completed\n",
    "                completed_files.add(txt_file.name)\n",
    "                \n",
    "                # Save progress after each file\n",
    "                with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump({\"completed_files\": list(completed_files)}, f, indent=2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"âŒ Error processing {txt_file}: {e}\")\n",
    "    \n",
    "    # Process both directories\n",
    "    print(\"\\nðŸ“‚ Extracting links from files...\")\n",
    "    # process_directory(posts_path, \"posts\")\n",
    "    process_directory(comments_path, \"comments\")\n",
    "    \n",
    "    # Count total links in CSV\n",
    "    total_links = 0\n",
    "    if csv_file.exists():\n",
    "        with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            total_links = sum(1 for line in f) - 1  # Subtract header\n",
    "    \n",
    "    print(f\"\\nâœ… Links saved successfully!\")\n",
    "    print(f\"   - Total links in CSV: {total_links}\")\n",
    "    print(f\"   - Total files processed: {len(completed_files)}\")\n",
    "    print(f\"   - Progress saved to: {progress_file}\")\n",
    "    \n",
    "    return completed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3e03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Extracting links from files...\n",
      "\n",
      "Processing 71 files in outputs\\comments (comments)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7413751ee0e449edabec578fba928da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing commentss:   0%|          | 0/71 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6315951411542ad810ceadfefd772d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in AdamParkhomenko_all_content.txt:   0%|          | 0/366 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a098fdf8d14441f8c92aa48395a5af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in ArielleScarcell_all_content.txt:   0%|          | 0/348 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95df1dfaf5dd4cebab2a875c61f4c1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in AstroTerry_all_content.txt:   0%|          | 0/311 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691948c5304b4b2c8f6967f38cfb558c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in bariweiss_all_content.txt:   0%|          | 0/219 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e67d73cd834012b41d2da32aa2f0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in blackintheempir_all_content.txt:   0%|          | 0/125 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8e4d82a0624b9890fec77ca0255bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in bresreports_all_content.txt:   0%|          | 0/136 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b302054e754660aee830d150364d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in brithume_all_content.txt:   0%|          | 0/449 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdb2f8c695c4b2484c4465ac7c8120c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in burgessev_all_content.txt:   0%|          | 0/39 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f8418ddb3f4bccbc3e8033dd548d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in ByYourLogic_all_content.txt:   0%|          | 0/159 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac03a689b534b1fae5d1c3b453930e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in CarlHigbie_all_content.txt:   0%|          | 0/576 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f0ab0ee5c743db874973ce6e22aa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in CensoredMen_all_content.txt:   0%|          | 0/335 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb992e1744514774924f12d9780d28d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in ChrisLoesch_all_content.txt:   0%|          | 0/135 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b892e17672db411bac1b91f1b194771d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in CollinRugg_all_content.txt:   0%|          | 0/264 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fd6807ebf34c3aaa6dc88be2fad358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in danpfeiffer_all_content.txt:   0%|          | 0/383 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d1c38dc9a5473eb7fb19fcb0a1a5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in danprimack_all_content.txt:   0%|          | 0/7 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6ff9e44d3c4967b8cb80197fc5d963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in dlacalle_IA_all_content.txt:   0%|          | 0/63 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1776eaa413cc43f99bfb369e1f561d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in DrLoupis_all_content.txt:   0%|          | 0/400 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12db2f33d4114e4d8b87c0898febd80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in elonmusk_all_content.txt:   0%|          | 0/647 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a313144a8bc04c5ba78adb32c1d4d1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in eveforamerica_all_content.txt:   0%|          | 0/163 [00:01<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda192b29edf4d5b9b0b7af0329debcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in FiorellaIsabelM_all_content.txt:   0%|          | 0/166 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b072a100424d7ab0f1d9792d4c0424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in FrankDangelo23_all_content.txt:   0%|          | 0/502 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4178d960d3974a529b4beffb072de132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in garethicke_all_content.txt:   0%|          | 0/374 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62c71fcda2f427294364c9d18056320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in GregRubini_all_content.txt:   0%|          | 0/837 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91ce22131bb402db1fc635998d7514e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in HilzFuld_all_content.txt:   0%|          | 0/201 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2358eec5d104705a86f4005028d5182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in IamBrookJackson_all_content.txt:   0%|          | 0/159 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074681299d604998a64bb57c8c2f3448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in jayrosen_nyu_all_content.txt:   0%|          | 0/30 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123ed0593d264743adb537837e764286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in jimsciutto_all_content.txt:   0%|          | 0/193 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96362502b5cd4a24a976783a7388f64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in JoeConchaTV_all_content.txt:   0%|          | 0/534 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c2773683ce4e7b8229e8f3e7d2b077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in JonahDispatch_all_content.txt:   0%|          | 0/143 [00:01<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7070f74d96a44de1bf85dab8d7e5369b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in JonathanTurley_all_content.txt:   0%|          | 0/51 [00:01<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8d82e4742d4ebf80b29479d02ca545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in JoshDenny_all_content.txt:   0%|          | 0/115 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a0339eb68e4bbd87d07af68d64ae15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in kacdnp91_all_content.txt:   0%|          | 0/188 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48160b2bd7e44af794f2e19e9b85cd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in KatiePhang_all_content.txt:   0%|          | 0/388 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975e7142ae284d93ac96bb56a988e486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in kristina_wong_all_content.txt:   0%|          | 0/501 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b709ff14415f4301b6230efe1c705939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in kyledcheney_all_content.txt:   0%|          | 0/142 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f4eeb52e3640788e4d6e2afc4e3616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in laurashin_all_content.txt:   0%|          | 0/78 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c28a67bae7644089c4ed46719e96d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in LEBassett_all_content.txt:   0%|          | 0/745 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282381690d884e89854240d3b3731130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in LeftAtLondon_all_content.txt: 0link [00:00, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd95dc4c2a2a4e2eb9646e41cbaa992e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in Leslieoo7_all_content.txt:   0%|          | 0/460 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0f2c2e26a44cc5a9b6749498102cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in LibertyCappy_all_content.txt:   0%|          | 0/116 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b756a026f4074109866e27c84abc816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in MarchandSurgery_all_content.txt:   0%|          | 0/576 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1088dceead2d45108aae1855ecaf3e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in MarkHertling_all_content.txt:   0%|          | 0/324 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bd904587694acb9ddbf1c8173e3903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in MaryLTrump_all_content.txt:   0%|          | 0/178 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9abb3ab9060479e8724778544d273e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in MattBruenig_all_content.txt:   0%|          | 0/109 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e3ffbb6d944fb790bf937ebf2a6564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in MikeASperrazza_all_content.txt:   0%|          | 0/350 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ef0f49ff514528b73aa013a9016e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in MikeSington_all_content.txt:   0%|          | 0/250 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c8675c3c0e489f91d47289743d8d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in molly0xFFF_all_content.txt:   0%|          | 0/83 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793566bd321145f4b7a53f8bde1316c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in MsAvaArmstrong_all_content.txt:   0%|          | 0/158 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e876c417718a4fe882778c17383be27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in NAChristakis_all_content.txt:   0%|          | 0/131 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fb1313605b4d02828aed0e80141e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in omgno2trump_all_content.txt:   0%|          | 0/376 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427c33eb10b0487587e051d62a1a37f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in Prolotario1_all_content.txt:   0%|          | 0/190 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e73ac8c47043529b614960f2309ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in Rach_IC_all_content.txt:   0%|          | 0/77 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fca31b632641a9b44497fff6a236f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in realDonaldTrump_all_content.txt:   0%|          | 0/2901 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3cf5567abf42b4b0dad3319e028abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in RightWingCope_all_content.txt:   0%|          | 0/504 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e880930f014a52b01a9967abcb3e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in RogerJStoneJr_all_content.txt:   0%|          | 0/57 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742f76fb4e6145cd8980c9c1866f9a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in saifedean_all_content.txt:   0%|          | 0/171 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c05bcabf0844a418194453b583c6c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in SamParkerSenate_all_content.txt:   0%|          | 0/348 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9445b15d33bf4fbaa99bd9e0f77973d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in SarahTheHaider_all_content.txt:   0%|          | 0/243 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b24326be104875ace27767aa321bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in secupp_all_content.txt:   0%|          | 0/408 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f603c29716de4b4eb844a1fc93fb4f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in simon_schama_all_content.txt:   0%|          | 0/419 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a16cd59426641f8930ae83afb9c4178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in StellaParton_all_content.txt:   0%|          | 0/81 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b766caa463c4779a29f091fc11e4a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in Tatarigami_UA_all_content.txt:   0%|          | 0/72 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146ed1f2fd50497a8c12dcc3eeb7aaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in thatdayin1992_all_content.txt:   0%|          | 0/124 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7d05c7c70b4a48947df84b4bbbd59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in TheBigMigShow_all_content.txt:   0%|          | 0/23 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c4039f28ef46c6bf15762daf768731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in thecoastguy_all_content.txt:   0%|          | 0/292 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c62593aada948a2b25cad78e9e8b8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in thejackhopkins_all_content.txt:   0%|          | 0/118 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad9581157334c1b9ddce152ef8d9c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in timinhonolulu_all_content.txt:   0%|          | 0/96 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daf78c0e0744e08bffc425b3031fd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in WBrettWilson_all_content.txt:   0%|          | 0/176 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64289f98f1814207b4e81b76fc0acfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in WhiteHouse_all_content.txt:   0%|          | 0/2148 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbabe7f11b0942ae8a273a3cb63c4b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in yarahawari_all_content.txt:   0%|          | 0/176 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e730647b0e8b46719d09825412c6be6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Resolving links in ylecun_all_content.txt:   0%|          | 0/241 [00:00<?, ?link/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Links saved successfully!\n",
      "   - Total links in CSV: 25548\n",
      "   - Total files processed: 71\n",
      "   - Progress saved to: outputs\\links\\progress.json\n"
     ]
    }
   ],
   "source": [
    "comments_path = \"outputs/comments\"\n",
    "posts_path = \"outputs/post\"\n",
    "output_links = \"outputs/links\"\n",
    "\n",
    "# Extract and resolve all links\n",
    "links_data = extract_and_resolve_links(comments_path, posts_path, output_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_links = \"outputs\\links\"\n",
    "# In all files txt in the comments_path and posts_path directories, we get all links starting with \"http\" and save them to file links.txt\n",
    "# save it as csv, one column \"link\", one column \"resolved_link\", one column \"category\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24755c",
   "metadata": {},
   "source": [
    "# EDA to process tweet redundant tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda9fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "# import emoji\n",
    "\n",
    "\n",
    "# def clean_and_extract_content(json_file_path, output_dir):\n",
    "#     json_file_path = Path(json_file_path)\n",
    "#     output_dir = Path(output_dir)\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     # Load JSON data\n",
    "#     with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     # Get username from filename\n",
    "#     author_username = json_file_path.stem\n",
    "    \n",
    "#     # Output files\n",
    "#     posts_file = output_dir / f\"{author_username}_posts_cleaned.txt\"\n",
    "#     comments_file = output_dir / f\"{author_username}_comments_cleaned.txt\"\n",
    "    \n",
    "#     def remove_tags(content, author=None, parent_author=None):\n",
    "#         \"\"\"\n",
    "#         Remove @mentions at the beginning of content.\n",
    "#         Removes author's own tag and parent comment author tag.\n",
    "#         \"\"\"\n",
    "#         if not content:\n",
    "#             return \"\"\n",
    "        \n",
    "#         # Remove the author's own mention\n",
    "#         if author:\n",
    "#             content = re.sub(rf'@{re.escape(author)}\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "#         # Remove parent author mention (for replies)\n",
    "#         if parent_author:\n",
    "#             content = re.sub(rf'@{re.escape(parent_author)}\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "#         # Remove any leading @mentions (general cleanup)\n",
    "#         content = re.sub(r'^(@\\w+\\s*)+', '', content)\n",
    "        \n",
    "#         return content.strip()\n",
    "    \n",
    "#     post_lines = []\n",
    "#     comment_lines = []\n",
    "\n",
    "#     def emoji_to_text(content):\n",
    "#         if not content:\n",
    "#             return \"\"\n",
    "        \n",
    "#         # Convert emoji to text with custom delimiters\n",
    "#         demojized = emoji.demojize(content, delimiters=(\" emo-\", \"- \"))\n",
    "        \n",
    "#         # Replace underscores with spaces in emoji names\n",
    "#         demojized = demojized.replace(\"_\", \" \")\n",
    "        \n",
    "#         return demojized\n",
    "    \n",
    "#     def hashtag_process(content):\n",
    "#         if not content:\n",
    "#             return \"\"\n",
    "        \n",
    "#         def split_hashtag(match):\n",
    "#             tag = match.group(1)\n",
    "#             # Add space before capital letters and numbers\n",
    "#             spaced = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', tag)\n",
    "#             spaced = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', spaced)\n",
    "#             return f\"hashtag-{spaced}-\"\n",
    "        \n",
    "#         content = re.sub(r'#(\\w+)', split_hashtag, content)\n",
    "#         return content\n",
    "    \n",
    "#     def extract_comment_content(comment, parent_author=None, depth=0):\n",
    "#         \"\"\"Recursively extract and clean comment content\"\"\"\n",
    "#         username = comment.get(\"username\")\n",
    "#         content = comment.get(\"content\", \"\")\n",
    "        \n",
    "#         # Clean the content\n",
    "#         no_tag_content = remove_tags(content, author=username, parent_author=parent_author)\n",
    "#         no_emoji_content = emoji_to_text(no_tag_content)\n",
    "#         no_hashtag_content = hashtag_process(no_emoji_content)\n",
    "\n",
    "#         cleaned_content = no_hashtag_content.strip()\n",
    "#         # Add to list if not empty\n",
    "#         if cleaned_content:\n",
    "#             # Collapse newlines to single line\n",
    "#             single_line = \" \".join(cleaned_content.splitlines())\n",
    "#             comment_lines.append(single_line)\n",
    "        \n",
    "#         # Process nested replies\n",
    "#         if \"replies_content\" in comment and comment[\"replies_content\"]:\n",
    "#             for reply in comment[\"replies_content\"]:\n",
    "#                 extract_comment_content(reply, parent_author=username, depth=depth + 1)\n",
    "    \n",
    "#     # Process each post\n",
    "#     for post in data:\n",
    "#         username = post.get(\"username\")\n",
    "#         content = post.get(\"content\", \"\")\n",
    "        \n",
    "#         # Clean post content (remove author's own tag)\n",
    "#         cleaned_content = remove_tags(content, author=username)\n",
    "        \n",
    "#         # Add to post lines if not empty\n",
    "#         if cleaned_content:\n",
    "#             single_line = \" \".join(cleaned_content.splitlines())\n",
    "#             post_lines.append(single_line)\n",
    "        \n",
    "#         # Process all comments for this post\n",
    "#         if \"replies_content\" in post and post[\"replies_content\"]:\n",
    "#             for comment in post[\"replies_content\"]:\n",
    "#                 # Pass post author as parent for top-level comments\n",
    "#                 extract_comment_content(comment, parent_author=username, depth=0)\n",
    "    \n",
    "#     # Save cleaned posts\n",
    "#     with open(posts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for line in post_lines:\n",
    "#             f.write(line + \"\\n\")\n",
    "    \n",
    "#     # Save cleaned comments\n",
    "#     with open(comments_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for line in comment_lines:\n",
    "#             f.write(line + \"\\n\")\n",
    "    \n",
    "#     print(f\"âœ… Cleaned content saved:\")\n",
    "#     print(f\"   - Posts: {posts_file} ({len(post_lines)} lines)\")\n",
    "#     print(f\"   - Comments: {comments_file} ({len(comment_lines)} lines)\")\n",
    "    \n",
    "#     return {\n",
    "#         \"posts_file\": posts_file,\n",
    "#         \"comments_file\": comments_file,\n",
    "#         \"posts_count\": len(post_lines),\n",
    "#         \"comments_count\": len(comment_lines)\n",
    "#     }\n",
    "\n",
    "# # Process all JSON files in a directory\n",
    "# def process_all_influencer_files(input_dir, output_dir):\n",
    "#     \"\"\"\n",
    "#     Process all JSON files in input directory and save cleaned content.\n",
    "    \n",
    "#     Args:\n",
    "#         input_dir: Directory containing JSON files\n",
    "#         output_dir: Directory to save cleaned content\n",
    "#     \"\"\"\n",
    "#     input_dir = Path(input_dir)\n",
    "#     json_files = list(input_dir.glob(\"*.json\"))\n",
    "    \n",
    "#     print(f\"Found {len(json_files)} JSON files to process\\n\")\n",
    "    \n",
    "#     results = []\n",
    "#     for json_file in json_files:\n",
    "#         print(f\"\\nðŸ“„ Processing: {json_file.name}\")\n",
    "#         try:\n",
    "#             result = clean_and_extract_content(json_file, output_dir)\n",
    "#             results.append(result)\n",
    "#         except Exception as e:\n",
    "#             print(f\"âŒ Error processing {json_file.name}: {e}\")\n",
    "    \n",
    "#     # Summary\n",
    "#     total_posts = sum(r[\"posts_count\"] for r in results)\n",
    "#     total_comments = sum(r[\"comments_count\"] for r in results)\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"ðŸ“Š Processing Complete!\")\n",
    "#     print(f\"   - Files processed: {len(results)}/{len(json_files)}\")\n",
    "#     print(f\"   - Total posts: {total_posts}\")\n",
    "#     print(f\"   - Total comments: {total_comments}\")\n",
    "#     print(f\"   - Output directory: {output_dir}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63751595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import emoji\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_and_extract_content_advanced(json_file_path, output_dir, links_csv_path):\n",
    "    \"\"\"\n",
    "    Advanced cleaning of content with comprehensive text processing.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to input JSON file\n",
    "        output_dir: Directory to save cleaned content\n",
    "        links_csv_path: Path to links CSV file for link resolution\n",
    "    \"\"\"\n",
    "    json_file_path = Path(json_file_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get username from filename\n",
    "    author_username = json_file_path.stem\n",
    "    \n",
    "    # Output files\n",
    "    posts_file = output_dir / f\"{author_username}_posts_cleaned.txt\"\n",
    "    comments_file = output_dir / f\"{author_username}_comments_cleaned.txt\"\n",
    "    corner_cases_file = output_dir / f\"{author_username}_corner_cases.txt\"\n",
    "    \n",
    "    # Load links mapping from CSV\n",
    "    links_map = {}\n",
    "    if Path(links_csv_path).exists():\n",
    "        with open(links_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                links_map[row[\"link\"]] = row[\"resolved_link\"]\n",
    "    \n",
    "    post_lines = []\n",
    "    comment_lines = []\n",
    "    corner_cases = []\n",
    "    \n",
    "    # Internet slang/shorthand mapping\n",
    "    slang_map = {\n",
    "        \"lol\": \"laugh out loud\",\n",
    "        \"lmao\": \"laughing\",\n",
    "        \"lmfao\": \"laughing\",\n",
    "        \"haha\": \"laughing\",\n",
    "        \"lmk\": \"let me know\",\n",
    "        \"rofl\": \"laughing\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"wtf\": \"what the fuck\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"imho\": \"in my humble opinion\",\n",
    "        \"tbh\": \"to be honest\",\n",
    "        \"smh\": \"shaking my head\",\n",
    "        \"fyi\": \"for your information\",\n",
    "        \"idk\": \"i do not know\",\n",
    "        \"irl\": \"in real life\",\n",
    "        \"brb\": \"be right back\",\n",
    "        \"af\": \"as fuck\",\n",
    "        \"rn\": \"right now\",\n",
    "        \"dm\": \"direct message\",\n",
    "        \"rt\": \"retweet\",\n",
    "        \"ngl\": \"not gonna lie\",\n",
    "        \"fr\": \"for real\",\n",
    "        \"bc\": \"because\",\n",
    "        \"tho\": \"though\",\n",
    "        \"thru\": \"through\",\n",
    "        \"ppl\": \"people\",\n",
    "        \"plz\": \"please\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"u\": \"you\",\n",
    "        \"ur\": \"your\",\n",
    "        \"y\": \"why\",\n",
    "        \"r\": \"are\",\n",
    "        \"b4\": \"before\",\n",
    "        \"gr8\": \"great\",\n",
    "        \"m8\": \"mate\",\n",
    "        \"2day\": \"today\",\n",
    "        \"2nite\": \"tonight\",\n",
    "        \"tmr\": \"tomorrow\",\n",
    "        \"tmrw\": \"tomorrow\",\n",
    "        \"asap\": \"as soon as possible\",\n",
    "        \"aka\": \"also known as\",\n",
    "        \"fomo\": \"fear of missing out\",\n",
    "        \"goat\": \"greatest of all time\",\n",
    "        \"sus\": \"suspicious\",\n",
    "        \"lowkey\": \"somewhat\",\n",
    "        \"highkey\": \"very\",\n",
    "        \"gtfo\": \"get the fuck out\",\n",
    "        \"stfu\": \"shut the fuck up\",\n",
    "        \"jk\": \"just kidding\",\n",
    "        \"ikr\": \"i know right\",\n",
    "        \"tfw\": \"that feeling when\",\n",
    "        \"mfw\": \"my face when\",\n",
    "        \"yolo\": \"you only live once\",\n",
    "        \"fam\": \"family\",\n",
    "        \"bro\": \"brother\",\n",
    "        \"sis\": \"sister\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"gotta\": \"got to\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"sorta\": \"sort of\",\n",
    "        \"dunno\": \"do not know\",\n",
    "        \"cuz\": \"because\",\n",
    "        \"w/\": \"with\",\n",
    "        \"w/o\": \"without\",\n",
    "        \"pov\": \"point of view\",\n",
    "        \"rip\": \"rest in peace\",\n",
    "        \"ftw\": \"for the win\",\n",
    "        \"gg\": \"good game\",\n",
    "        \"wp\": \"well played\",\n",
    "        \"ez\": \"easy\",\n",
    "        \"op\": \"overpowered\",\n",
    "        \"af\": \"as fuck\",\n",
    "        \"lit\": \"amazing\",\n",
    "        \"salty\": \"upset\",\n",
    "        \"savage\": \"brutal\",\n",
    "        \"flex\": \"show off\",\n",
    "        \"stan\": \"support\",\n",
    "        \"simp\": \"obsessed\",\n",
    "        \"based\": \"authentic\",\n",
    "        \"ratio\": \"more engagement\",\n",
    "        \"copium\": \"denial\",\n",
    "        \"hopium\": \"hope\",\n",
    "        \"oof\": \"expression of pain\",\n",
    "        \"yeet\": \"throw\",\n",
    "        \"bet\": \"okay\",\n",
    "        \"cap\": \"lie\",\n",
    "        \"no cap\": \"no lie\",\n",
    "        \"bussin\": \"really good\",\n",
    "        \"slaps\": \"really good\",\n",
    "        \"hits different\": \"unique experience\",\n",
    "        \"vibe\": \"feeling\",\n",
    "        \"mood\": \"relatable\",\n",
    "        \"same\": \"i agree\",\n",
    "        \"facts\": \"truth\",\n",
    "        \"periodt\": \"end of discussion\",\n",
    "        \"tea\": \"gossip\",\n",
    "        \"spill the tea\": \"share gossip\",\n",
    "        \"shade\": \"disrespect\",\n",
    "        \"throw shade\": \"insult\",\n",
    "        \"slay\": \"do great\",\n",
    "        \"queen\": \"amazing woman\",\n",
    "        \"king\": \"amazing man\",\n",
    "        \"w\": \"win\",\n",
    "        \"l\": \"loss\"\n",
    "    }\n",
    "    \n",
    "    # Common words to remove (stop words and contractions)\n",
    "    remove_words = {\n",
    "        \"'s\", \"'re\", \"'ll\", \"'ve\", \"'d\", \"'m\",\n",
    "        \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\",\n",
    "        \"of\", \"with\", \"by\", \"from\", \"as\", \"is\", \"was\", \"are\", \"were\", \"be\",\n",
    "        \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\",\n",
    "        \"will\", \"would\", \"should\", \"could\", \"may\", \"might\", \"must\",\n",
    "        \"can\", \"shall\", \"this\", \"that\", \"these\", \"those\"\n",
    "    }\n",
    "    \n",
    "    # Negative terms to replace with \"not\"\n",
    "    negative_terms = {\n",
    "        \"not\", \"dont\", \"don't\", \"wont\", \"won't\", \"isnt\", \"isn't\",\n",
    "        \"arent\", \"aren't\", \"wasnt\", \"wasn't\", \"werent\", \"weren't\",\n",
    "        \"hasnt\", \"hasn't\", \"havent\", \"haven't\", \"hadnt\", \"hadn't\",\n",
    "        \"doesnt\", \"doesn't\", \"didnt\", \"didn't\", \"wouldnt\", \"wouldn't\",\n",
    "        \"shouldnt\", \"shouldn't\", \"couldnt\", \"couldn't\", \"cant\", \"can't\",\n",
    "        \"mustnt\", \"mustn't\", \"no\", \"never\", \"neither\", \"nobody\", \"nothing\", \"nowhere\"\n",
    "    }\n",
    "    \n",
    "    def normalize_repeated_chars(text):\n",
    "        \"\"\"\n",
    "        Normalize repeated characters to max 2 occurrences.\n",
    "        Examples: wahhhhhh -> wahh, gooooood -> good\n",
    "        \"\"\"\n",
    "        # Replace 3+ repeated characters with max 2\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    def is_media_link(url):\n",
    "        \"\"\"Check if URL is media (photo/video)\"\"\"\n",
    "        media_indicators = [\"/photo/\", \"/video/\", \"/status/\", \"twimg.com\", \"pbs.twimg.com\"]\n",
    "        return any(indicator in url.lower() for indicator in media_indicators)\n",
    "    \n",
    "    def classify_number(text, number):\n",
    "        \"\"\"Classify if number is year/date/time or generic number\"\"\"\n",
    "        try:\n",
    "            # Year pattern (1900-2099)\n",
    "            if 1900 <= int(number) <= 2099:\n",
    "                return \"year\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # Date patterns (MM/DD, DD-MM, etc.)\n",
    "        if re.search(r'\\d{1,2}[/-]\\d{1,2}', text):\n",
    "            return \"date\"\n",
    "        \n",
    "        # Time patterns (HH:MM)\n",
    "        if re.search(r'\\d{1,2}:\\d{2}', text):\n",
    "            return \"time\"\n",
    "        \n",
    "        return \"number\"\n",
    "    \n",
    "    def clean_text(content, author=None, parent_author=None):\n",
    "        \"\"\"\n",
    "        Comprehensive text cleaning pipeline.\n",
    "        \"\"\"\n",
    "        if not content:\n",
    "            return \"\", []\n",
    "        \n",
    "        original_content = content\n",
    "        has_non_ascii = False\n",
    "        non_ascii_words = []\n",
    "        \n",
    "        # Step 1: Remove RT and following @mention\n",
    "        content = re.sub(r'^RT\\s+@\\w+:\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 2: Remove author's own mention\n",
    "        if author:\n",
    "            content = re.sub(rf'@{re.escape(author)}\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 3: Remove parent author mention (for replies)\n",
    "        if parent_author:\n",
    "            content = re.sub(rf'@{re.escape(parent_author)}\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 4: Remove leading @mentions\n",
    "        content = re.sub(r'^(@\\w+\\s*)+', '', content)\n",
    "        \n",
    "        # Step 5: Replace remaining @mentions with \"tag\"\n",
    "        content = re.sub(r'@\\w+', 'tag', content)\n",
    "        \n",
    "        # Step 6: Process links\n",
    "        def replace_link(match):\n",
    "            url = match.group(0).rstrip('.,;:!?)')\n",
    "            resolved_url = links_map.get(url, url)\n",
    "            \n",
    "            if is_media_link(resolved_url):\n",
    "                return \"media\"\n",
    "            else:\n",
    "                return \"link\"\n",
    "        \n",
    "        content = re.sub(r'https?://[^\\s]+', replace_link, content)\n",
    "        \n",
    "        # Step 7: Process hashtags - remove # and add spaces\n",
    "        def process_hashtag(match):\n",
    "            tag = match.group(1)\n",
    "            # Add space before capital letters and numbers\n",
    "            spaced = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', tag)\n",
    "            spaced = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', spaced)\n",
    "            return spaced.lower()\n",
    "        \n",
    "        content = re.sub(r'#(\\w+)', process_hashtag, content)\n",
    "        \n",
    "        # Step 8: Replace emojis with \"emoji\"\n",
    "        content = emoji.replace_emoji(content, replace='')\n",
    "        \n",
    "        # Step 9: Replace numbers with suffixes (7k, 213m, 5b, etc.) with \"number\"\n",
    "        content = re.sub(r'\\b\\d+\\.?\\d*[kmbtKMBT]\\b', 'number', content)\n",
    "        \n",
    "        # Step 10: Process regular numbers\n",
    "        def replace_number(match):\n",
    "            num = match.group(0)\n",
    "            num_type = classify_number(content, num)\n",
    "            return num_type\n",
    "        \n",
    "        content = re.sub(r'\\b\\d+\\b', replace_number, content)\n",
    "        \n",
    "        # Step 11: Normalize repeated characters\n",
    "        content = normalize_repeated_chars(content)\n",
    "        \n",
    "        # Step 12: Translate internet slang/shorthand\n",
    "        words = content.split()\n",
    "        translated_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            # Remove punctuation for slang matching\n",
    "            word_clean = re.sub(r'[^\\w\\s-]', '', word_lower)\n",
    "            \n",
    "            # Check if it's slang\n",
    "            if word_clean in slang_map:\n",
    "                translated_words.append(slang_map[word_clean])\n",
    "            else:\n",
    "                translated_words.append(word)\n",
    "        \n",
    "        content = ' '.join(translated_words)\n",
    "        \n",
    "        # Step 13: Handle contractions and negative terms\n",
    "        words = content.split()\n",
    "        cleaned_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            \n",
    "            # Check for non-ASCII characters\n",
    "            if not all(ord(c) < 128 for c in word):\n",
    "                has_non_ascii = True\n",
    "                non_ascii_words.append(word)\n",
    "                continue  # Skip non-ASCII words\n",
    "            \n",
    "            # Replace negative terms with \"not\"\n",
    "            if word_lower in negative_terms:\n",
    "                cleaned_words.append(\"not\")\n",
    "            # Remove common stop words\n",
    "            elif word_lower not in remove_words:\n",
    "                # Remove special characters except hyphens in middle of words\n",
    "                cleaned_word = re.sub(r'[^\\w\\s-]', '', word_lower)\n",
    "                if cleaned_word and cleaned_word not in remove_words:\n",
    "                    cleaned_words.append(cleaned_word)\n",
    "        \n",
    "        # Step 14: Join and normalize spaces\n",
    "        content = ' '.join(cleaned_words)\n",
    "        content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        content = content.lower()\n",
    "        \n",
    "        # Track corner cases\n",
    "        corner_case_info = []\n",
    "        if has_non_ascii:\n",
    "            corner_case_info.append({\n",
    "                \"original\": original_content,\n",
    "                \"non_ascii_words\": non_ascii_words,\n",
    "                \"cleaned\": content\n",
    "            })\n",
    "        \n",
    "        return content, corner_case_info\n",
    "    \n",
    "    def extract_comment_content(comment, parent_author=None, depth=0):\n",
    "        \"\"\"Recursively extract and clean comment content\"\"\"\n",
    "        username = comment.get(\"username\")\n",
    "        content = comment.get(\"content\", \"\")\n",
    "        \n",
    "        # Clean the content\n",
    "        cleaned_content, cases = clean_text(content, author=username, parent_author=parent_author)\n",
    "        \n",
    "        # Track corner cases\n",
    "        corner_cases.extend(cases)\n",
    "        \n",
    "        # Add to list if not empty\n",
    "        if cleaned_content:\n",
    "            comment_lines.append(cleaned_content)\n",
    "        \n",
    "        # Process nested replies\n",
    "        if \"replies_content\" in comment and comment[\"replies_content\"]:\n",
    "            for reply in comment[\"replies_content\"]:\n",
    "                extract_comment_content(reply, parent_author=username, depth=depth + 1)\n",
    "    \n",
    "    # Process each post\n",
    "    for post in data:\n",
    "        username = post.get(\"username\")\n",
    "        content = post.get(\"content\", \"\")\n",
    "        \n",
    "        # Clean post content\n",
    "        cleaned_content, cases = clean_text(content, author=username)\n",
    "        \n",
    "        # Track corner cases\n",
    "        corner_cases.extend(cases)\n",
    "        \n",
    "        # Add to post lines if not empty\n",
    "        if cleaned_content:\n",
    "            post_lines.append(cleaned_content)\n",
    "        \n",
    "        # Process all comments for this post\n",
    "        if \"replies_content\" in post and post[\"replies_content\"]:\n",
    "            for comment in post[\"replies_content\"]:\n",
    "                extract_comment_content(comment, parent_author=username, depth=0)\n",
    "    \n",
    "    # Save cleaned posts\n",
    "    with open(posts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in post_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    # Save cleaned comments\n",
    "    with open(comments_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in comment_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    # Save corner cases\n",
    "    if corner_cases:\n",
    "        with open(corner_cases_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for case in corner_cases:\n",
    "                f.write(f\"Original: {case['original']}\\n\")\n",
    "                f.write(f\"Non-ASCII words: {', '.join(case['non_ascii_words'])}\\n\")\n",
    "                f.write(f\"Cleaned: {case['cleaned']}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    print(f\"âœ… Cleaned content saved:\")\n",
    "    print(f\"   - Posts: {posts_file} ({len(post_lines)} lines)\")\n",
    "    print(f\"   - Comments: {comments_file} ({len(comment_lines)} lines)\")\n",
    "    print(f\"   - Corner cases: {corner_cases_file} ({len(corner_cases)} cases)\")\n",
    "    \n",
    "    return {\n",
    "        \"posts_file\": posts_file,\n",
    "        \"comments_file\": comments_file,\n",
    "        \"posts_count\": len(post_lines),\n",
    "        \"comments_count\": len(comment_lines),\n",
    "        \"corner_cases_count\": len(corner_cases)\n",
    "    }\n",
    "\n",
    "# Process all JSON files in a directory\n",
    "def process_all_influencer_files_advanced(input_dir, output_dir, links_csv_path):\n",
    "    \"\"\"\n",
    "    Process all JSON files with advanced cleaning.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    json_files = list(input_dir.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for json_file in json_files:\n",
    "        print(f\"\\nðŸ“„ Processing: {json_file.name}\")\n",
    "        try:\n",
    "            result = clean_and_extract_content_advanced(json_file, output_dir, links_csv_path)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {json_file.name}: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    total_posts = sum(r[\"posts_count\"] for r in results)\n",
    "    total_comments = sum(r[\"comments_count\"] for r in results)\n",
    "    total_corner_cases = sum(r[\"corner_cases_count\"] for r in results)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š Processing Complete!\")\n",
    "    print(f\"   - Files processed: {len(results)}/{len(json_files)}\")\n",
    "    print(f\"   - Total posts: {total_posts}\")\n",
    "    print(f\"   - Total comments: {total_comments}\")\n",
    "    print(f\"   - Total corner cases: {total_corner_cases}\")\n",
    "    print(f\"   - Output directory: {output_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b70c33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\AdamParkhomenko_posts_cleaned.txt (40 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\AdamParkhomenko_comments_cleaned.txt (1637 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\AdamParkhomenko_corner_cases.txt (320 cases)\n"
     ]
    }
   ],
   "source": [
    "EXPR_DIR = \"influencer_data/AdamParkhomenko.json\"\n",
    "OUTPUT_DIR = \"outputs/cleaned_content_advanced\"\n",
    "LINKS_CSV_PATH = \"outputs/links/links.csv\"\n",
    "\n",
    "# Process single file\n",
    "result = clean_and_extract_content_advanced(\n",
    "    EXPR_DIR, \n",
    "    OUTPUT_DIR, \n",
    "    LINKS_CSV_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f93e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_text(content):\n",
    "    if not content:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert emoji to text with custom delimiters\n",
    "    demojized = emoji.demojize(content, delimiters=(\" emo-\", \"- \"))\n",
    "    \n",
    "    # Replace underscores with spaces in emoji names\n",
    "    demojized = demojized.replace(\"_\", \" \")\n",
    "    \n",
    "    return demojized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c8ed519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lol, you kept asking, trying to get it to prove you right. But, you're not, so it never gave what you were looking for. Have you learned anything from this? Embarrassing  emo-face with tears of joy-  emo-face with tears of joy-  emo-face with tears of joy- \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_to_text(\"Lol, you kept asking, trying to get it to prove you right. But, you're not, so it never gave what you were looking for. Have you learned anything from this? Embarrassing ðŸ˜‚ðŸ˜‚ðŸ˜‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419d6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import emoji\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "\n",
    "def clean_and_extract_content_advanced(json_file_path, output_dir, links_csv_path):\n",
    "    \"\"\"\n",
    "    Advanced cleaning of content with comprehensive text processing.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to input JSON file\n",
    "        output_dir: Directory to save cleaned content\n",
    "        links_csv_path: Path to links CSV file for link resolution\n",
    "    \"\"\"\n",
    "    json_file_path = Path(json_file_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get username from filename\n",
    "    author_username = json_file_path.stem\n",
    "    \n",
    "    # Output files\n",
    "    posts_file = output_dir / f\"{author_username}_posts_cleaned.txt\"\n",
    "    comments_file = output_dir / f\"{author_username}_comments_cleaned.txt\"\n",
    "    corner_cases_file = output_dir / \"corner_cases.txt\"\n",
    "    \n",
    "    # Load links mapping from CSV\n",
    "    links_map = {}\n",
    "    if Path(links_csv_path).exists():\n",
    "        with open(links_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                links_map[row[\"link\"]] = row[\"resolved_link\"]\n",
    "    \n",
    "    post_lines = []\n",
    "    comment_lines = []\n",
    "    corner_cases = []\n",
    "    \n",
    "    # US State abbreviations mapping\n",
    "    us_states = {\n",
    "        \"AL\": \"alabama\", \"AK\": \"alaska\", \"AZ\": \"arizona\", \"AR\": \"arkansas\",\n",
    "        \"CA\": \"california\", \"CO\": \"colorado\", \"CT\": \"connecticut\", \"DE\": \"delaware\",\n",
    "        \"FL\": \"florida\", \"GA\": \"georgia\", \"HI\": \"hawaii\", \"ID\": \"idaho\",\n",
    "        \"IL\": \"illinois\", \"IN\": \"indiana\", \"IA\": \"iowa\", \"KS\": \"kansas\",\n",
    "        \"KY\": \"kentucky\", \"LA\": \"louisiana\", \"ME\": \"maine\", \"MD\": \"maryland\",\n",
    "        \"MA\": \"massachusetts\", \"MI\": \"michigan\", \"MN\": \"minnesota\", \"MS\": \"mississippi\",\n",
    "        \"MO\": \"missouri\", \"MT\": \"montana\", \"NE\": \"nebraska\", \"NV\": \"nevada\",\n",
    "        \"NH\": \"new hampshire\", \"NJ\": \"new jersey\", \"NM\": \"new mexico\", \"NY\": \"new york\",\n",
    "        \"NC\": \"north carolina\", \"ND\": \"north dakota\", \"OH\": \"ohio\", \"OK\": \"oklahoma\",\n",
    "        \"OR\": \"oregon\", \"PA\": \"pennsylvania\", \"RI\": \"rhode island\", \"SC\": \"south carolina\",\n",
    "        \"SD\": \"south dakota\", \"TN\": \"tennessee\", \"TX\": \"texas\", \"UT\": \"utah\",\n",
    "        \"VT\": \"vermont\", \"VA\": \"virginia\", \"WA\": \"washington\", \"WV\": \"west virginia\",\n",
    "        \"WI\": \"wisconsin\", \"WY\": \"wyoming\", \"DC\": \"washington dc\"\n",
    "    }\n",
    "    \n",
    "    # Country code mapping (ISO 3166-1 alpha-2 and alpha-3)\n",
    "    countries = {\n",
    "        \"US\": \"united states\", \"USA\": \"united states\",\n",
    "        \"UK\": \"united kingdom\", \"GB\": \"united kingdom\", \"GBR\": \"united kingdom\",\n",
    "        \"CA\": \"canada\", \"CAN\": \"canada\",\n",
    "        \"CN\": \"china\", \"CHN\": \"china\",\n",
    "        \"JP\": \"japan\", \"JPN\": \"japan\",\n",
    "        \"DE\": \"germany\", \"DEU\": \"germany\",\n",
    "        \"FR\": \"france\", \"FRA\": \"france\",\n",
    "        \"IT\": \"italy\", \"ITA\": \"italy\",\n",
    "        \"ES\": \"spain\", \"ESP\": \"spain\",\n",
    "        \"AU\": \"australia\", \"AUS\": \"australia\",\n",
    "        \"BR\": \"brazil\", \"BRA\": \"brazil\",\n",
    "        \"IN\": \"india\", \"IND\": \"india\",\n",
    "        \"RU\": \"russia\", \"RUS\": \"russia\",\n",
    "        \"MX\": \"mexico\", \"MEX\": \"mexico\",\n",
    "        \"KR\": \"south korea\", \"KOR\": \"south korea\",\n",
    "        \"AR\": \"argentina\", \"ARG\": \"argentina\",\n",
    "        \"ZA\": \"south africa\", \"ZAF\": \"south africa\",\n",
    "        \"SA\": \"saudi arabia\", \"SAU\": \"saudi arabia\",\n",
    "        \"TR\": \"turkey\", \"TUR\": \"turkey\",\n",
    "        \"PL\": \"poland\", \"POL\": \"poland\",\n",
    "        \"NL\": \"netherlands\", \"NLD\": \"netherlands\",\n",
    "        \"BE\": \"belgium\", \"BEL\": \"belgium\",\n",
    "        \"SE\": \"sweden\", \"SWE\": \"sweden\",\n",
    "        \"NO\": \"norway\", \"NOR\": \"norway\",\n",
    "        \"DK\": \"denmark\", \"DNK\": \"denmark\",\n",
    "        \"FI\": \"finland\", \"FIN\": \"finland\",\n",
    "        \"IE\": \"ireland\", \"IRL\": \"ireland\",\n",
    "        \"PT\": \"portugal\", \"PRT\": \"portugal\",\n",
    "        \"GR\": \"greece\", \"GRC\": \"greece\",\n",
    "        \"CH\": \"switzerland\", \"CHE\": \"switzerland\",\n",
    "        \"AT\": \"austria\", \"AUT\": \"austria\",\n",
    "        \"NZ\": \"new zealand\", \"NZL\": \"new zealand\",\n",
    "        \"SG\": \"singapore\", \"SGP\": \"singapore\",\n",
    "        \"HK\": \"hong kong\", \"HKG\": \"hong kong\",\n",
    "        \"IL\": \"israel\", \"ISR\": \"israel\",\n",
    "        \"EG\": \"egypt\", \"EGY\": \"egypt\",\n",
    "        \"NG\": \"nigeria\", \"NGA\": \"nigeria\",\n",
    "        \"KE\": \"kenya\", \"KEN\": \"kenya\",\n",
    "        \"PH\": \"philippines\", \"PHL\": \"philippines\",\n",
    "        \"TH\": \"thailand\", \"THA\": \"thailand\",\n",
    "        \"VN\": \"vietnam\", \"VNM\": \"vietnam\",\n",
    "        \"MY\": \"malaysia\", \"MYS\": \"malaysia\",\n",
    "        \"ID\": \"indonesia\", \"IDN\": \"indonesia\",\n",
    "        \"PK\": \"pakistan\", \"PAK\": \"pakistan\",\n",
    "        \"BD\": \"bangladesh\", \"BGD\": \"bangladesh\",\n",
    "        \"UA\": \"ukraine\", \"UKR\": \"ukraine\"\n",
    "    }\n",
    "    \n",
    "    # Internet slang/shorthand mapping\n",
    "    slang_map = {\n",
    "        \"lol\": \"laugh out loud\",\n",
    "        \"lmao\": \"laughing\",\n",
    "        \"lmfao\": \"laughing\",\n",
    "        \"haha\": \"laughing\",\n",
    "        \"lmk\": \"let me know\",\n",
    "        \"rofl\": \"laughing\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"wtf\": \"what the fuck\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"imho\": \"in my humble opinion\",\n",
    "        \"tbh\": \"to be honest\",\n",
    "        \"smh\": \"shaking my head\",\n",
    "        \"fyi\": \"for your information\",\n",
    "        \"idk\": \"i do not know\",\n",
    "        \"irl\": \"in real life\",\n",
    "        \"brb\": \"be right back\",\n",
    "        \"af\": \"as fuck\",\n",
    "        \"rn\": \"right now\",\n",
    "        \"dm\": \"direct message\",\n",
    "        \"rt\": \"retweet\",\n",
    "        \"ngl\": \"not gonna lie\",\n",
    "        \"fr\": \"for real\",\n",
    "        \"bc\": \"because\",\n",
    "        \"tho\": \"though\",\n",
    "        \"thru\": \"through\",\n",
    "        \"ppl\": \"people\",\n",
    "        \"plz\": \"please\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"u\": \"you\",\n",
    "        \"ur\": \"your\",\n",
    "        \"y\": \"why\",\n",
    "        \"r\": \"are\",\n",
    "        \"b4\": \"before\",\n",
    "        \"gr8\": \"great\",\n",
    "        \"m8\": \"mate\",\n",
    "        \"2day\": \"today\",\n",
    "        \"2nite\": \"tonight\",\n",
    "        \"tmr\": \"tomorrow\",\n",
    "        \"tmrw\": \"tomorrow\",\n",
    "        \"asap\": \"as soon as possible\",\n",
    "        \"aka\": \"also known as\",\n",
    "        \"fomo\": \"fear of missing out\",\n",
    "        \"goat\": \"greatest of all time\",\n",
    "        \"sus\": \"suspicious\",\n",
    "        \"lowkey\": \"somewhat\",\n",
    "        \"highkey\": \"very\",\n",
    "        \"gtfo\": \"get the fuck out\",\n",
    "        \"stfu\": \"shut the fuck up\",\n",
    "        \"jk\": \"just kidding\",\n",
    "        \"ikr\": \"i know right\",\n",
    "        \"tfw\": \"that feeling when\",\n",
    "        \"mfw\": \"my face when\",\n",
    "        \"yolo\": \"you only live once\",\n",
    "        \"fam\": \"family\",\n",
    "        \"bro\": \"brother\",\n",
    "        \"sis\": \"sister\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"gotta\": \"got to\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"sorta\": \"sort of\",\n",
    "        \"dunno\": \"do not know\",\n",
    "        \"cuz\": \"because\",\n",
    "        \"w/\": \"with\",\n",
    "        \"w/o\": \"without\",\n",
    "        \"pov\": \"point of view\",\n",
    "        \"rip\": \"rest in peace\",\n",
    "        \"ftw\": \"for the win\",\n",
    "        \"gg\": \"good game\",\n",
    "        \"wp\": \"well played\",\n",
    "        \"ez\": \"easy\",\n",
    "        \"op\": \"overpowered\",\n",
    "        \"af\": \"as fuck\",\n",
    "        \"lit\": \"amazing\",\n",
    "        \"salty\": \"upset\",\n",
    "        \"savage\": \"brutal\",\n",
    "        \"flex\": \"show off\",\n",
    "        \"stan\": \"support\",\n",
    "        \"simp\": \"obsessed\",\n",
    "        \"based\": \"authentic\",\n",
    "        \"ratio\": \"more engagement\",\n",
    "        \"copium\": \"denial\",\n",
    "        \"hopium\": \"hope\",\n",
    "        \"oof\": \"expression of pain\",\n",
    "        \"yeet\": \"throw\",\n",
    "        \"bet\": \"okay\",\n",
    "        \"cap\": \"lie\",\n",
    "        \"no cap\": \"no lie\",\n",
    "        \"bussin\": \"really good\",\n",
    "        \"slaps\": \"really good\",\n",
    "        \"hits different\": \"unique experience\",\n",
    "        \"vibe\": \"feeling\",\n",
    "        \"mood\": \"relatable\",\n",
    "        \"same\": \"i agree\",\n",
    "        \"facts\": \"truth\",\n",
    "        \"periodt\": \"end of discussion\",\n",
    "        \"tea\": \"gossip\",\n",
    "        \"spill the tea\": \"share gossip\",\n",
    "        \"shade\": \"disrespect\",\n",
    "        \"throw shade\": \"insult\",\n",
    "        \"slay\": \"do great\",\n",
    "        \"queen\": \"amazing woman\",\n",
    "        \"king\": \"amazing man\",\n",
    "        \"w\": \"win\",\n",
    "        \"l\": \"loss\"\n",
    "    }\n",
    "\n",
    "    # Negative contractions -> subject + not\n",
    "    contractions_negative = {\n",
    "        \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "        \"won't\": \"will not\", \"wouldn't\": \"would not\", \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\", \"can't\": \"can not\", \"cannot\": \"can not\",\n",
    "        \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "        \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\",\n",
    "        \"mustn't\": \"must not\", \"mightn't\": \"might not\", \"needn't\": \"need not\",\n",
    "        \"shan't\": \"shall not\", \"daren't\": \"dare not\"\n",
    "    }\n",
    "    \n",
    "    # Positive contractions - remove auxiliary/modal verbs\n",
    "    contractions_positive = {\n",
    "        \"i'm\": \"i\", \"you're\": \"you\", \"he's\": \"he\", \"she's\": \"she\", \n",
    "        \"it's\": \"it\", \"we're\": \"we\", \"they're\": \"they\",\n",
    "        \"i'll\": \"i\", \"you'll\": \"you\", \"he'll\": \"he\", \"she'll\": \"she\",\n",
    "        \"it'll\": \"it\", \"we'll\": \"we\", \"they'll\": \"they\",\n",
    "        \"i've\": \"i\", \"you've\": \"you\", \"we've\": \"we\", \"they've\": \"they\",\n",
    "        \"i'd\": \"i\", \"you'd\": \"you\", \"he'd\": \"he\", \"she'd\": \"she\",\n",
    "        \"it'd\": \"it\", \"we'd\": \"we\", \"they'd\": \"they\"\n",
    "    }\n",
    "    \n",
    "    # Common words to remove (stop words and contractions)\n",
    "    remove_words = {\n",
    "        \"'s\", \"'re\", \"'ll\", \"'ve\", \"'d\", \"'m\",\n",
    "        \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\",\n",
    "        \"of\", \"with\", \"by\", \"from\", \"as\", \"is\", \"was\", \"are\", \"were\", \"be\",\n",
    "        \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\",\n",
    "        \"will\", \"would\", \"should\", \"could\", \"may\", \"might\", \"must\",\n",
    "        \"can\", \"shall\", \"this\", \"that\", \"these\", \"those\"\n",
    "    }\n",
    "    \n",
    "    # Negative terms to replace with \"not\"\n",
    "    negative_terms = {\n",
    "        \"not\", \"dont\", \"don't\", \"wont\", \"won't\", \"isnt\", \"isn't\",\n",
    "        \"arent\", \"aren't\", \"wasnt\", \"wasn't\", \"werent\", \"weren't\",\n",
    "        \"hasnt\", \"hasn't\", \"havent\", \"haven't\", \"hadnt\", \"hadn't\",\n",
    "        \"doesnt\", \"doesn't\", \"didnt\", \"didn't\", \"wouldnt\", \"wouldn't\",\n",
    "        \"shouldnt\", \"shouldn't\", \"couldnt\", \"couldn't\", \"cant\", \"can't\",\n",
    "        \"mustnt\", \"mustn't\", \"no\", \"never\", \"neither\", \"nobody\", \"nothing\", \"nowhere\"\n",
    "    }\n",
    "\n",
    "    # Text-based emoji/kaomoji dictionary (for future use)\n",
    "    text_emoji_map = {\n",
    "        r\"Â¯\\\\_\\(ãƒ„\\)_/Â¯\": \"shrug\",\n",
    "        r\"\\(ãƒ„\\)\": \"happy face\",\n",
    "        r\"à² _à² \": \"disapproval\",\n",
    "        r\"à² ç›Šà² \": \"angry\",\n",
    "        r\"Ê˜â€¿Ê˜\": \"innocent\",\n",
    "        r\"\\(â•¯Â°â–¡Â°\\)â•¯ï¸µ â”»â”â”»\": \"table flip\",\n",
    "        r\"â”¬â”€â”¬ãƒŽ\\( Âº _ ÂºãƒŽ\\)\": \"table respect\",\n",
    "        r\"á••\\( á› \\)á•—\": \"excited\",\n",
    "        r\"\\(^_^\\)\": \"happy\",\n",
    "        r\"\\^_\\^\": \"happy\",\n",
    "        r\"ï¼¼\\(^o^\\)ï¼\": \"celebrating\",\n",
    "        r\"Ù©\\(â—•â€¿â—•ï½¡\\)Û¶\": \"excited\",\n",
    "        r\"\\(ã¥ï½¡â—•â€¿â€¿â—•ï½¡\\)ã¥\": \"hug\",\n",
    "        r\"áƒš\\(à² ç›Šà² áƒš\\)\": \"why\",\n",
    "        r\"Â¯\\\\_\\(ã‚·\\)_/Â¯\": \"shrug\",\n",
    "        r\"\\(âŒâ– _â– \\)\": \"cool\",\n",
    "        r\"\\(â•¥_â•¥\\)\": \"crying\",\n",
    "        r\"\\(T_T\\)\": \"crying\",\n",
    "        r\"\\._.\": \"sad\",\n",
    "        r\";_;\": \"crying\",\n",
    "        r\"T_T\": \"crying\",\n",
    "        r\"o_o\": \"surprised\",\n",
    "        r\"O_O\": \"shocked\",\n",
    "        r\">_<\": \"frustrated\",\n",
    "        r\"^_^\": \"happy\",\n",
    "        r\"XD\": \"laughing hard\",\n",
    "        r\"xD\": \"laughing\",\n",
    "        r\":D\": \"big smile\",\n",
    "        r\"=D\": \"big smile\",\n",
    "        r\":]\": \"smile\",\n",
    "        r\":[\": \"sad\",\n",
    "        r\":(\": \"sad\",\n",
    "        r\":)\": \"smile\",\n",
    "        r\";)\": \"wink\",\n",
    "        r\";D\": \"wink laugh\",\n",
    "    }\n",
    "\n",
    "    # Special Unicode symbols to remove (when standalone)\n",
    "    standalone_symbols = {\n",
    "        \"â€¢\",  # Bullet point\n",
    "        \"â—¦\",  # White bullet\n",
    "        \"â–ª\",  # Black small square\n",
    "        \"â–«\",  # White small square\n",
    "        \"Â·\",  # Middle dot\n",
    "        \"â€»\",  # Reference mark\n",
    "        \"â†’\",  # Right arrow\n",
    "        \"â†\",  # Left arrow\n",
    "        \"â†‘\",  # Up arrow\n",
    "        \"â†“\",  # Down arrow\n",
    "        \"âœ“\",  # Check mark\n",
    "        \"âœ—\",  # X mark\n",
    "        \"â˜…\",  # Star\n",
    "        \"â˜†\",  # White star\n",
    "        \"â™¦\",  # Diamond\n",
    "        \"â™¥\",  # Heart\n",
    "        \"â™ \",  # Spade\n",
    "        \"â™£\",  # Club\n",
    "    }\n",
    "    \n",
    "    def normalize_repeated_chars(text):\n",
    "        \"\"\"\n",
    "        Normalize repeated characters to max 2 occurrences.\n",
    "        Examples: wahhhhhh -> wahh, gooooood -> good\n",
    "        \"\"\"\n",
    "        # Replace 3+ repeated characters with max 2\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    def is_media_link(url):\n",
    "        \"\"\"Check if URL is media (photo/video)\"\"\"\n",
    "        media_indicators = [\"/photo/\", \"/video/\", \"/status/\", \"twimg.com\", \"pbs.twimg.com\"]\n",
    "        return any(indicator in url.lower() for indicator in media_indicators)\n",
    "    \n",
    "    def classify_number(text, number):\n",
    "        \"\"\"Classify if number is year/date/time or generic number\"\"\"\n",
    "        try:\n",
    "            # Year pattern (1900-2099)\n",
    "            if 1900 <= int(number) <= 2099:\n",
    "                return \"year\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # Date patterns (MM/DD, DD-MM, etc.)\n",
    "        if re.search(r'\\d{1,2}[/-]\\d{1,2}', text):\n",
    "            return \"date\"\n",
    "        \n",
    "        # Time patterns (HH:MM)\n",
    "        if re.search(r'\\d{1,2}:\\d{2}', text):\n",
    "            return \"time\"\n",
    "        \n",
    "        return \"number\"\n",
    "    \n",
    "    def clean_text(content, author=None, parent_author=None):\n",
    "        \"\"\"\n",
    "        Comprehensive text cleaning pipeline.\n",
    "        \"\"\"\n",
    "        if not content:\n",
    "            return \"\", []\n",
    "        \n",
    "        original_content = content\n",
    "        has_non_ascii = False\n",
    "        non_ascii_words = []\n",
    "        \n",
    "        # Step 0: Convert Unicode characters to closest ASCII equivalents using unidecode\n",
    "        # This handles accented characters like estÃ¡ â†’ esta, polÃ­tica â†’ politica\n",
    "        content = unidecode(content)\n",
    "        \n",
    "        # Step 0.5: Remove specific characters from words (keep the rest of the word)\n",
    "        # Remove these characters wherever they appear\n",
    "        chars_to_strip = ['(', ')', '[', ']', '{', '}', '!', '?', '|', '*', '~', '`', '^', '<', '>', 'Â¡', 'â€¢']\n",
    "        for char in chars_to_strip:\n",
    "            content = content.replace(char, '')\n",
    "        \n",
    "        # Remove leading/trailing special characters from each word (-, +, =, _, etc.)\n",
    "        # This preserves hyphens in the middle of words like \"well-known\"\n",
    "        content = re.sub(r'\\b[-+=_]+', '', content)  # Remove at start of word\n",
    "        content = re.sub(r'[-+=_]+\\b', '', content)  # Remove at end of word\n",
    "        \n",
    "        # Step 1: Remove RT and following @mention\n",
    "        content = re.sub(r'^RT\\s+@\\w+:\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 2: Remove author's own mention\n",
    "        if author:\n",
    "            content = re.sub(rf'@{re.escape(author)}\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 3: Remove parent author mention (for replies)\n",
    "        if parent_author:\n",
    "            content = re.sub(rf'@{re.escape(parent_author)}\\s*', '', content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 4: Remove leading @mentions\n",
    "        content = re.sub(r'^(@\\w+\\s*)+', '', content)\n",
    "        \n",
    "        # Step 5: Replace remaining @mentions with \"tag\"\n",
    "        content = re.sub(r'@\\w+', 'tag', content)\n",
    "        \n",
    "        # Step 6: Process links\n",
    "        def replace_link(match):\n",
    "            url = match.group(0).rstrip('.,;:!?)')\n",
    "            resolved_url = links_map.get(url, url)\n",
    "            \n",
    "            if is_media_link(resolved_url):\n",
    "                return \"media\"\n",
    "            else:\n",
    "                return \"link\"\n",
    "        \n",
    "        content = re.sub(r'https?://[^\\s]+', replace_link, content)\n",
    "        \n",
    "        # Step 7: Process hashtags - remove # and add spaces\n",
    "        def process_hashtag(match):\n",
    "            tag = match.group(1)\n",
    "            # Add space before capital letters and numbers\n",
    "            spaced = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', tag)\n",
    "            spaced = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', spaced)\n",
    "            return spaced.lower()\n",
    "        \n",
    "        content = re.sub(r'#(\\w+)', process_hashtag, content)\n",
    "        \n",
    "        # Step 8: Replace emojis (remove them)\n",
    "        content = emoji.replace_emoji(content, replace='')\n",
    "        \n",
    "        # Step 8.5: Replace non-ASCII apostrophes and quotes with standard versions\n",
    "        # Apostrophes: ' (U+2019), ` (U+0060), Ê¼ (U+02BC), Â´ (U+00B4), ' (U+2018)\n",
    "        content = re.sub(r\"[\\u2019\\u0060\\u02BC\\u00B4\\u2018]\", \"'\", content)\n",
    "        \n",
    "        # Quotes: \" \" (U+201C, U+201D), â€ž (U+201E), \" (U+201F)\n",
    "        content = re.sub(r\"[\\u201C\\u201D\\u201E\\u201F]\", '\"', content)\n",
    "        \n",
    "        # Dashes/hyphens: â€” (em dash U+2014), â€“ (en dash U+2013), â€ (hyphen U+2010)\n",
    "        content = re.sub(r\"[\\u2014\\u2013\\u2010]\", \"-\", content)\n",
    "        \n",
    "        # Ellipsis: â€¦ (U+2026) and multiple dots\n",
    "        content = re.sub(r\"\\u2026|\\.{2,}\", \" \", content)\n",
    "        \n",
    "        # Remove quotes (both single and double)\n",
    "        content = content.replace('\"', '').replace(\"'\", '')\n",
    "        \n",
    "        # Step 8.7: Remove standalone special symbols (bullets, arrows, etc.) - ONLY when standalone\n",
    "        # These are only removed when they appear alone, not as part of a word\n",
    "        for symbol in standalone_symbols:\n",
    "            content = re.sub(rf'\\s{re.escape(symbol)}\\s', ' ', content)\n",
    "            content = re.sub(rf'^{re.escape(symbol)}\\s', '', content)\n",
    "            content = re.sub(rf'\\s{re.escape(symbol)}$', '', content)\n",
    "        \n",
    "        # Step 8.6: Handle contractions - expand them properly\n",
    "        words = content.split()\n",
    "        expanded_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            word_clean = re.sub(r'[^\\w\\s\\'-]', '', word_lower)\n",
    "            \n",
    "            if word_clean in contractions_negative:\n",
    "                expanded_words.append(contractions_negative[word_clean])\n",
    "            elif word_clean in contractions_positive:\n",
    "                expanded_words.append(contractions_positive[word_clean])\n",
    "            else:\n",
    "                expanded_words.append(word)\n",
    "        \n",
    "        content = ' '.join(expanded_words)\n",
    "        \n",
    "        # Step 9.1: Replace numbers with suffixes (7k, 213m, 5b, etc.) with \"number\"\n",
    "        content = re.sub(r'\\b\\d+\\.?\\d*[kmbtKMBT]\\b', 'number', content)\n",
    "        # Step 9.2: Replace numbers with commas (e.g., 1,000, 1.000) with \"number\"\n",
    "        content = re.sub(r'\\b\\d{1,3}(,\\d{3})+(\\.\\d+)?\\b', 'number', content)\n",
    "        # Step 9.3: Replace ordered numbers (1st, 2nd, 3rd, 4th, etc.) with \"number\"\n",
    "        content = re.sub(r'\\b\\d+(st|nd|rd|th)\\b', 'number', content)\n",
    "        \n",
    "        # Step 9.4: Expand US state abbreviations (must be word boundaries)\n",
    "        for abbr, full_name in us_states.items():\n",
    "            # Use word boundaries to avoid matching parts of words\n",
    "            content = re.sub(rf'\\b{abbr}\\b', full_name, content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 9.5: Expand country codes (prioritize 3-letter codes first to avoid conflicts)\n",
    "        for code, full_name in sorted(countries.items(), key=lambda x: len(x[0]), reverse=True):\n",
    "            content = re.sub(rf'\\b{code}\\b', full_name, content, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Step 10: Process regular numbers\n",
    "        def replace_number(match):\n",
    "            num = match.group(0)\n",
    "            num_type = classify_number(content, num)\n",
    "            return num_type\n",
    "        \n",
    "        content = re.sub(r'\\b\\d+\\b', replace_number, content)\n",
    "        \n",
    "        # Step 11: Normalize repeated characters\n",
    "        content = normalize_repeated_chars(content)\n",
    "        \n",
    "        # Step 12: Translate internet slang/shorthand\n",
    "        words = content.split()\n",
    "        translated_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            # Remove punctuation for slang matching\n",
    "            word_clean = re.sub(r'[^\\w\\s-]', '', word_lower)\n",
    "            \n",
    "            # Check if it's slang\n",
    "            if word_clean in slang_map:\n",
    "                translated_words.append(slang_map[word_clean])\n",
    "            else:\n",
    "                translated_words.append(word)\n",
    "        \n",
    "        content = ' '.join(translated_words)\n",
    "        \n",
    "        # Step 13: Handle remaining text\n",
    "        words = content.split()\n",
    "        cleaned_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            \n",
    "            # Check for non-ASCII characters (after unidecode, this should be rare)\n",
    "            if not all(ord(c) < 128 for c in word):\n",
    "                has_non_ascii = True\n",
    "                non_ascii_words.append(word)\n",
    "                continue  # Skip non-ASCII words\n",
    "            \n",
    "            # Replace negative terms with \"not\"\n",
    "            if word_lower in negative_terms:\n",
    "                cleaned_words.append(\"not\")\n",
    "            # Remove common stop words\n",
    "            elif word_lower not in remove_words:\n",
    "                # Remove remaining special characters except hyphens in middle of words\n",
    "                # This keeps \"well-known\" but removes \"word.\" or \".word\"\n",
    "                cleaned_word = re.sub(r'^[^\\w]+|[^\\w]+$', '', word_lower)\n",
    "                cleaned_word = re.sub(r'[^\\w\\s-]', '', cleaned_word)\n",
    "                \n",
    "                if cleaned_word and cleaned_word not in remove_words:\n",
    "                    cleaned_words.append(cleaned_word)\n",
    "        \n",
    "        # Step 14: Join and normalize spaces\n",
    "        content = ' '.join(cleaned_words)\n",
    "        content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        content = content.lower()\n",
    "        \n",
    "        # Track corner cases\n",
    "        corner_case_info = []\n",
    "        if has_non_ascii:\n",
    "            corner_case_info.append({\n",
    "                \"original\": original_content,\n",
    "                \"non_ascii_words\": non_ascii_words,\n",
    "                \"cleaned\": content\n",
    "            })\n",
    "        \n",
    "        return content, corner_case_info\n",
    "    \n",
    "    def extract_comment_content(comment, parent_author=None, depth=0):\n",
    "        \"\"\"Recursively extract and clean comment content\"\"\"\n",
    "        username = comment.get(\"username\")\n",
    "        content = comment.get(\"content\", \"\")\n",
    "        \n",
    "        # Clean the content\n",
    "        cleaned_content, cases = clean_text(content, author=username, parent_author=parent_author)\n",
    "        \n",
    "        # Track corner cases\n",
    "        corner_cases.extend(cases)\n",
    "        \n",
    "        # Add to list if not empty\n",
    "        if cleaned_content:\n",
    "            comment_lines.append(cleaned_content)\n",
    "        \n",
    "        # Process nested replies\n",
    "        if \"replies_content\" in comment and comment[\"replies_content\"]:\n",
    "            for reply in comment[\"replies_content\"]:\n",
    "                extract_comment_content(reply, parent_author=username, depth=depth + 1)\n",
    "    \n",
    "    # Process each post\n",
    "    for post in data:\n",
    "        username = post.get(\"username\")\n",
    "        content = post.get(\"content\", \"\")\n",
    "        \n",
    "        # Clean post content\n",
    "        cleaned_content, cases = clean_text(content, author=username)\n",
    "        \n",
    "        # Track corner cases\n",
    "        corner_cases.extend(cases)\n",
    "        \n",
    "        # Add to post lines if not empty\n",
    "        if cleaned_content:\n",
    "            post_lines.append(cleaned_content)\n",
    "        \n",
    "        # Process all comments for this post\n",
    "        if \"replies_content\" in post and post[\"replies_content\"]:\n",
    "            for comment in post[\"replies_content\"]:\n",
    "                extract_comment_content(comment, parent_author=username, depth=0)\n",
    "    \n",
    "    # Save cleaned posts\n",
    "    with open(posts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in post_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    # Save cleaned comments\n",
    "    with open(comments_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in comment_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    # Save corner cases\n",
    "    if corner_cases:\n",
    "        with open(corner_cases_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            for case in corner_cases:\n",
    "                f.write(f\"Original: {case['original']}\\n\")\n",
    "                f.write(f\"Non-ASCII words: {', '.join(case['non_ascii_words'])}\\n\")\n",
    "                f.write(f\"Cleaned: {case['cleaned']}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    print(f\"âœ… Cleaned content saved:\")\n",
    "    print(f\"   - Posts: {posts_file} ({len(post_lines)} lines)\")\n",
    "    print(f\"   - Comments: {comments_file} ({len(comment_lines)} lines)\")\n",
    "    print(f\"   - Corner cases: {corner_cases_file} ({len(corner_cases)} cases)\")\n",
    "    \n",
    "    return {\n",
    "        \"posts_file\": posts_file,\n",
    "        \"comments_file\": comments_file,\n",
    "        \"posts_count\": len(post_lines),\n",
    "        \"comments_count\": len(comment_lines),\n",
    "        \"corner_cases_count\": len(corner_cases)\n",
    "    }\n",
    "\n",
    "# Process all JSON files in a directory\n",
    "def process_all_influencer_files_advanced(input_dir, output_dir, links_csv_path):\n",
    "    \"\"\"\n",
    "    Process all JSON files with advanced cleaning.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    json_files = list(input_dir.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for json_file in json_files:\n",
    "        print(f\"\\nðŸ“„ Processing: {json_file.name}\")\n",
    "        try:\n",
    "            result = clean_and_extract_content_advanced(json_file, output_dir, links_csv_path)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {json_file.name}: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    total_posts = sum(r[\"posts_count\"] for r in results)\n",
    "    total_comments = sum(r[\"comments_count\"] for r in results)\n",
    "    total_corner_cases = sum(r[\"corner_cases_count\"] for r in results)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š Processing Complete!\")\n",
    "    print(f\"   - Files processed: {len(results)}/{len(json_files)}\")\n",
    "    print(f\"   - Total posts: {total_posts}\")\n",
    "    print(f\"   - Total comments: {total_comments}\")\n",
    "    print(f\"   - Total corner cases: {total_corner_cases}\")\n",
    "    print(f\"   - Output directory: {output_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5f6ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 JSON files to process\n",
      "\n",
      "\n",
      "ðŸ“„ Processing: AdamParkhomenko.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\AdamParkhomenko_posts_cleaned.txt (40 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\AdamParkhomenko_comments_cleaned.txt (1572 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: ArielleScarcell.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\ArielleScarcell_posts_cleaned.txt (33 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\ArielleScarcell_comments_cleaned.txt (2336 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: AstroTerry.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\AstroTerry_posts_cleaned.txt (32 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\AstroTerry_comments_cleaned.txt (1599 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: bariweiss.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\bariweiss_posts_cleaned.txt (39 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\bariweiss_comments_cleaned.txt (1342 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: blackintheempir.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\blackintheempir_posts_cleaned.txt (27 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\blackintheempir_comments_cleaned.txt (451 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: bresreports.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\bresreports_posts_cleaned.txt (35 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\bresreports_comments_cleaned.txt (1278 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: brithume.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\brithume_posts_cleaned.txt (46 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\brithume_comments_cleaned.txt (3393 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: burgessev.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\burgessev_posts_cleaned.txt (26 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\burgessev_comments_cleaned.txt (429 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: ByYourLogic.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\ByYourLogic_posts_cleaned.txt (42 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\ByYourLogic_comments_cleaned.txt (1220 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: CarlHigbie.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\CarlHigbie_posts_cleaned.txt (37 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\CarlHigbie_comments_cleaned.txt (3566 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: CensoredMen.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\CensoredMen_posts_cleaned.txt (26 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\CensoredMen_comments_cleaned.txt (2116 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: ChrisLoesch.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\ChrisLoesch_posts_cleaned.txt (42 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\ChrisLoesch_comments_cleaned.txt (795 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: CollinRugg.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\CollinRugg_posts_cleaned.txt (21 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\CollinRugg_comments_cleaned.txt (2215 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: danpfeiffer.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\danpfeiffer_posts_cleaned.txt (47 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\danpfeiffer_comments_cleaned.txt (2638 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: danprimack.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\danprimack_posts_cleaned.txt (27 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\danprimack_comments_cleaned.txt (123 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: dlacalle_IA.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\dlacalle_IA_posts_cleaned.txt (27 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\dlacalle_IA_comments_cleaned.txt (451 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: DrLoupis.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\DrLoupis_posts_cleaned.txt (43 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\DrLoupis_comments_cleaned.txt (1715 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: elonmusk.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\elonmusk_posts_cleaned.txt (44 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\elonmusk_comments_cleaned.txt (5076 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: eveforamerica.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\eveforamerica_posts_cleaned.txt (31 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\eveforamerica_comments_cleaned.txt (1069 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: FiorellaIsabelM.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\FiorellaIsabelM_posts_cleaned.txt (35 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\FiorellaIsabelM_comments_cleaned.txt (1147 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: FrankDangelo23.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\FrankDangelo23_posts_cleaned.txt (45 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\FrankDangelo23_comments_cleaned.txt (2576 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: garethicke.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\garethicke_posts_cleaned.txt (36 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\garethicke_comments_cleaned.txt (2186 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: GregRubini.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\GregRubini_posts_cleaned.txt (40 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\GregRubini_comments_cleaned.txt (5200 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: HilzFuld.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\HilzFuld_posts_cleaned.txt (26 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\HilzFuld_comments_cleaned.txt (962 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: IamBrookJackson.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\IamBrookJackson_posts_cleaned.txt (38 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\IamBrookJackson_comments_cleaned.txt (843 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: jayrosen_nyu.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\jayrosen_nyu_posts_cleaned.txt (26 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\jayrosen_nyu_comments_cleaned.txt (437 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: jimsciutto.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\jimsciutto_posts_cleaned.txt (32 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\jimsciutto_comments_cleaned.txt (1628 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: JoeConchaTV.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\JoeConchaTV_posts_cleaned.txt (37 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\JoeConchaTV_comments_cleaned.txt (3021 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: JonahDispatch.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\JonahDispatch_posts_cleaned.txt (42 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\JonahDispatch_comments_cleaned.txt (928 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: JonathanTurley.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\JonathanTurley_posts_cleaned.txt (21 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\JonathanTurley_comments_cleaned.txt (892 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: JoshDenny.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\JoshDenny_posts_cleaned.txt (23 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\JoshDenny_comments_cleaned.txt (577 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: kacdnp91.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\kacdnp91_posts_cleaned.txt (29 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\kacdnp91_comments_cleaned.txt (809 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: KatiePhang.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\KatiePhang_posts_cleaned.txt (43 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\KatiePhang_comments_cleaned.txt (2018 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: kristina_wong.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\kristina_wong_posts_cleaned.txt (41 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\kristina_wong_comments_cleaned.txt (2544 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: kyledcheney.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\kyledcheney_posts_cleaned.txt (26 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\kyledcheney_comments_cleaned.txt (1265 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: laurashin.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\laurashin_posts_cleaned.txt (35 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\laurashin_comments_cleaned.txt (666 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: LEBassett.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\LEBassett_posts_cleaned.txt (41 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\LEBassett_comments_cleaned.txt (4643 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: LeftAtLondon.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\LeftAtLondon_posts_cleaned.txt (2 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\LeftAtLondon_comments_cleaned.txt (1 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: Leslieoo7.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\Leslieoo7_posts_cleaned.txt (36 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\Leslieoo7_comments_cleaned.txt (2109 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: LibertyCappy.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\LibertyCappy_posts_cleaned.txt (30 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\LibertyCappy_comments_cleaned.txt (575 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: MarchandSurgery.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\MarchandSurgery_posts_cleaned.txt (33 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\MarchandSurgery_comments_cleaned.txt (3131 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: MarkHertling.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\MarkHertling_posts_cleaned.txt (32 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\MarkHertling_comments_cleaned.txt (2069 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: MaryLTrump.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\MaryLTrump_posts_cleaned.txt (20 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\MaryLTrump_comments_cleaned.txt (986 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: MattBruenig.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\MattBruenig_posts_cleaned.txt (38 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\MattBruenig_comments_cleaned.txt (783 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: MikeASperrazza.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\MikeASperrazza_posts_cleaned.txt (27 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\MikeASperrazza_comments_cleaned.txt (1856 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: MikeSington.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\MikeSington_posts_cleaned.txt (21 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\MikeSington_comments_cleaned.txt (1352 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: molly0xFFF.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\molly0xFFF_posts_cleaned.txt (23 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\molly0xFFF_comments_cleaned.txt (520 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: MsAvaArmstrong.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\MsAvaArmstrong_posts_cleaned.txt (40 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\MsAvaArmstrong_comments_cleaned.txt (1180 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: NAChristakis.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\NAChristakis_posts_cleaned.txt (33 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\NAChristakis_comments_cleaned.txt (1083 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: omgno2trump.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\omgno2trump_posts_cleaned.txt (31 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\omgno2trump_comments_cleaned.txt (1631 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: Prolotario1.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\Prolotario1_posts_cleaned.txt (33 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\Prolotario1_comments_cleaned.txt (1799 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: Rach_IC.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\Rach_IC_posts_cleaned.txt (36 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\Rach_IC_comments_cleaned.txt (701 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: realDonaldTrump.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\realDonaldTrump_posts_cleaned.txt (22 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\realDonaldTrump_comments_cleaned.txt (13390 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: RightWingCope.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\RightWingCope_posts_cleaned.txt (29 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\RightWingCope_comments_cleaned.txt (3120 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: RogerJStoneJr.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\RogerJStoneJr_posts_cleaned.txt (29 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\RogerJStoneJr_comments_cleaned.txt (569 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: saifedean.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\saifedean_posts_cleaned.txt (42 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\saifedean_comments_cleaned.txt (1117 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: SamParkerSenate.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\SamParkerSenate_posts_cleaned.txt (38 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\SamParkerSenate_comments_cleaned.txt (2248 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: SarahTheHaider.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\SarahTheHaider_posts_cleaned.txt (39 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\SarahTheHaider_comments_cleaned.txt (1907 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: secupp.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\secupp_posts_cleaned.txt (39 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\secupp_comments_cleaned.txt (2786 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: simon_schama.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\simon_schama_posts_cleaned.txt (41 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\simon_schama_comments_cleaned.txt (2282 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: StellaParton.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\StellaParton_posts_cleaned.txt (17 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\StellaParton_comments_cleaned.txt (792 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: Tatarigami_UA.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\Tatarigami_UA_posts_cleaned.txt (26 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\Tatarigami_UA_comments_cleaned.txt (536 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: thatdayin1992.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\thatdayin1992_posts_cleaned.txt (29 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\thatdayin1992_comments_cleaned.txt (842 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: TheBigMigShow.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\TheBigMigShow_posts_cleaned.txt (28 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\TheBigMigShow_comments_cleaned.txt (291 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: thecoastguy.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\thecoastguy_posts_cleaned.txt (38 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\thecoastguy_comments_cleaned.txt (1894 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: thejackhopkins.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\thejackhopkins_posts_cleaned.txt (36 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\thejackhopkins_comments_cleaned.txt (729 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: timinhonolulu.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\timinhonolulu_posts_cleaned.txt (41 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\timinhonolulu_comments_cleaned.txt (612 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: WBrettWilson.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\WBrettWilson_posts_cleaned.txt (37 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\WBrettWilson_comments_cleaned.txt (1271 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: WhiteHouse.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\WhiteHouse_posts_cleaned.txt (184 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\WhiteHouse_comments_cleaned.txt (11104 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: yarahawari.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\yarahawari_posts_cleaned.txt (38 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\yarahawari_comments_cleaned.txt (1020 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "ðŸ“„ Processing: ylecun.json\n",
      "âœ… Cleaned content saved:\n",
      "   - Posts: outputs\\cleaned_content_advanced\\ylecun_posts_cleaned.txt (44 lines)\n",
      "   - Comments: outputs\\cleaned_content_advanced\\ylecun_comments_cleaned.txt (1632 lines)\n",
      "   - Corner cases: outputs\\cleaned_content_advanced\\corner_cases.txt (0 cases)\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Processing Complete!\n",
      "   - Files processed: 71/71\n",
      "   - Total posts: 2513\n",
      "   - Total comments: 133644\n",
      "   - Total corner cases: 0\n",
      "   - Output directory: outputs/cleaned_content_advanced\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Process single file\n",
    "EXPR_DIR = \"influencer_data/AdamParkhomenko.json\"\n",
    "OUTPUT_DIR = \"outputs/cleaned_content_advanced\"\n",
    "LINKS_CSV_PATH = \"outputs/links/links.csv\"\n",
    "\n",
    "# result = clean_and_extract_content_advanced(\n",
    "#     EXPR_DIR, \n",
    "#     OUTPUT_DIR, \n",
    "#     LINKS_CSV_PATH\n",
    "# )\n",
    "\n",
    "# Process all files\n",
    "results = process_all_influencer_files_advanced(\n",
    "    \"influencer_data\",\n",
    "    \"outputs/cleaned_content_advanced\",\n",
    "    \"outputs/links/links.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
